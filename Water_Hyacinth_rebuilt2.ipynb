{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "42e4b74d",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GielHagenbeek/WaterHyacinths/blob/main/Water_Hyacinth_classification_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b38af25",
      "metadata": {
        "id": "TLKzlYnn-v0h"
      },
      "source": [
        "\n",
        "Water Hyacinth classification tool:\n",
        "Copyright 2020 Niels Janssens\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
        "\n",
        "http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac8c3804",
      "metadata": {
        "id": "40IUO0o2_cmZ"
      },
      "source": [
        "## 1. Install all required modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57bc0d6e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XmPgFgh37S1b",
        "outputId": "ab2fbc6d-2822-4865-efa9-ffa61b910d68"
      },
      "outputs": [],
      "source": [
        "# install required modules\n",
        "!pip install pandas\n",
        "!pip install geopandas\n",
        "!pip install rasterio\n",
        "!pip install sentinelhub\n",
        "!pip install eo-learn\n",
        "!pip install gdal\n",
        "!pip install cartopy\n",
        "!pip install shapely\n",
        "!pip install fiona\n",
        "!pip install geos\n",
        "!pip install proj\n",
        "!pip install scikit-image\n",
        "!pip install geoviews'\n",
        "!pip install lightgbm\n",
        "!pip install s2cloudless #import cloudless model - clean perhaps\n",
        "!pip install eo-learn graphviz\n",
        "\n",
        "!apt-get install -qq libgdal-dev libproj-dev\n",
        "!pip install cartopy\n",
        "!pip install eo-learn-visualization[FULL] #clean perhaps\n",
        "\n",
        "!pip install numpy\n",
        "!pip install pyepsg\n",
        "!pip install -U scikit-image\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9dfbff5",
      "metadata": {
        "id": "Y7YSXt38_ESx"
      },
      "source": [
        "## 2. Import required models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "681e9a1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J81tmmAjvaGr",
        "outputId": "256e7960-409e-4342-80db-fb27cc6aac4f"
      },
      "outputs": [],
      "source": [
        "# import required modules\n",
        "# Built-in modules\n",
        "import pickle\n",
        "import os\n",
        "import datetime\n",
        "import itertools\n",
        "from enum import Enum\n",
        "from functools import partial\n",
        "from datetime import timedelta\n",
        "from datetime import datetime\n",
        "import scipy.stats as stats\n",
        "import copy\n",
        "\n",
        "# Basics of Python data handling and visualization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "from numba import jit\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.colors as mcolors\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from shapely.geometry import Polygon as Polygon1\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import shapely\n",
        "import pyepsg\n",
        "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
        "from google.colab import drive\n",
        "\n",
        "# Machine learning\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import sklearn.model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Imports from eo-learn and sentinelhub-py\n",
        "from sentinelhub import (\n",
        "    CRS,\n",
        "    BBox,\n",
        "    BBoxSplitter,\n",
        "    CustomGridSplitter,\n",
        "    DataCollection,\n",
        "    MimeType,\n",
        "    MosaickingOrder,\n",
        "    OsmSplitter,\n",
        "    SentinelHubDownloadClient,\n",
        "    SentinelHubRequest,\n",
        "    TileSplitter,\n",
        "    UtmGridSplitter,\n",
        "    UtmZoneSplitter,\n",
        "    read_data,\n",
        ")\n",
        "from eolearn.core import SaveTask, LoadTask, FeatureType, EOWorkflow, EOTask, OverwritePermission, EOExecutor, EOPatch,EONode, OutputTask, AddFeatureTask\n",
        "from eolearn.io import SentinelHubInputTask, SentinelHubDemTask, SentinelHubEvalscriptTask, get_available_timestamps, ExportToTiffTask\n",
        "from eolearn.mask import MaskFeatureTask, JoinMasksTask\n",
        "from eolearn.mask.extra.cloud_mask import CloudMaskTask #Add cloud mask task\n",
        "from eolearn.geometry import VectorToRasterTask, ErosionTask #PointSamplingTask replaced?\n",
        "from eolearn.features import SimpleFilterTask\n",
        "import eolearn.features.extra.interpolation\n",
        "from eolearn.features.extra.interpolation import LinearInterpolationTask\n",
        "import eolearn.visualization\n",
        "import skimage.transform\n",
        "from skimage.filters import threshold_otsu, sobel\n",
        "from skimage.morphology import disk, erosion, dilation, opening, closing, white_tophat"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f96e05",
      "metadata": {
        "id": "UCro36Hb_QYz"
      },
      "source": [
        "## 3. Configure Sentinelhub settings & connect to Google Drive folder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb88444a",
      "metadata": {
        "id": "oNudvxsUpWB9"
      },
      "source": [
        "### 3.1. Connect to Sentinelhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bc436b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2awkJwl7dlx",
        "outputId": "8a5bbcaa-2010-4fca-95a4-be11224f78e3"
      },
      "outputs": [],
      "source": [
        "\"\"\"insert sentinelhub configuration settings here. Create a Sentinel-2 configuration on\n",
        "   https://apps.sentinel-hub.com/dashboard/#/ and replace the empty \"\" instance_id,\n",
        "   sh_client_id and sh_client_secret by your custom strings\"\"\"\n",
        "\n",
        "!sentinelhub.config --instance_id \"\"\n",
        "!sentinelhub.config --sh_client_id \"\"\n",
        "!sentinelhub.config --sh_client_secret \"\"\n",
        "!sentinelhub.config --max_download_attempts 15\n",
        "!sentinelhub.config --download_sleep_time 300\n",
        "!sentinelhub.config --max_wfs_records_per_query 10\n",
        "!sentinelhub.config --max_opensearch_records_per_query 10\n",
        "!sentinelhub.config --show\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81ec088",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLAQ5_kF23kn",
        "outputId": "d3a30427-1f8d-4f64-a7b0-45e0432e9c74"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3341bfa6",
      "metadata": {
        "id": "RxZrDuU4_vjC"
      },
      "source": [
        "## 4. Test and train manually labeled Water Hyacinth and Water for supervised\n",
        "classification pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d13ec0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yQkqUmCdPUD",
        "outputId": "e33c6cfe-dbac-43fa-85f3-b88c2f19e668"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# read manually labeled pixel data that can be used as input for the Naive Bayes classifier\n",
        "data = pd.read_csv('./drive/My Drive/SentinelModel/input_data/points_all.csv')\n",
        "np.take(data,np.random.permutation(data.shape[0]),axis=0)\n",
        "\n",
        "# split testing and training data using the sklearn module\n",
        "testing_data, training_data = sklearn.model_selection.train_test_split(data)\n",
        "\n",
        "# convert classes to an integer\n",
        "training_classes_int = np.zeros(training_data[\"Label\"].values.shape, dtype=np.uint8)\n",
        "training_classes_int[training_data[\"Label\"].values == \"WH\"] = 2\n",
        "training_classes_int[training_data[\"Label\"].values == \"W\"] = 1\n",
        "\n",
        "# convert classes to an integer\n",
        "testing_classes_int = np.zeros(testing_data[\"Label\"].values.shape, dtype=np.uint8)\n",
        "testing_classes_int[testing_data[\"Label\"].values == \"WH\"]   = 2\n",
        "testing_classes_int[testing_data[\"Label\"].values == \"W\"] = 1\n",
        "\n",
        "# convert to one numpy array\n",
        "training_array = np.array([training_classes_int, training_data[\"ndvi\"].values, training_data[\"fai\"].values,\n",
        "                              training_data[\"B2\"].values, training_data[\"B3\"].values, training_data[\"B12\"].values])\n",
        "\n",
        "# flip around\n",
        "training_array = training_array.transpose()\n",
        "\n",
        "# convert to one numpy array\n",
        "testing_array = np.array([testing_classes_int, testing_data[\"ndvi\"].values, testing_data[\"fai\"].values,\n",
        "                             testing_data[\"B2\"].values, testing_data[\"B3\"].values, testing_data[\"B12\"].values])\n",
        "\n",
        "# flip around\n",
        "testing_array = testing_array.transpose()\n",
        "\n",
        "# train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "class_model = gnb.fit(training_array[:,1:], training_array[:,0])\n",
        "\n",
        "# Check fit. Note as this uses exactly the same data as training doesn't represent accuracy.\n",
        "# It does give a rough idea of how well model fits though\n",
        "score = class_model.score(testing_array[:,1:], testing_array[:,0])\n",
        "print(\"Classification fit using all data: {:.02}\".format(score))\n",
        "\n",
        "\n",
        "# predict classes by using the trained model\n",
        "out_classes = class_model.predict(testing_array[:,1:])\n",
        "\n",
        "\n",
        "# uncomment to save the model\n",
        "joblib.dump(class_model, './model_SI_LULC.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0249f718",
      "metadata": {
        "id": "qvJ1p5aqHaIv"
      },
      "outputs": [],
      "source": [
        "# group data by Label, rename the bands for visualisation purposes\n",
        "mean_values = data.groupby('Label', as_index=False)[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']].mean()\n",
        "std_values = data.groupby('Label', as_index=False)[['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']].std()\n",
        "mean_values.rename(columns={'B1': '442', 'B2': '492', 'B3': '559', 'B4': '665', 'B5': '704', 'B6': '740', 'B7': '780', 'B8': '833', 'B8A': '864', 'B9': '944', 'B11': '1612', 'B12': '2190'}, inplace=True)\n",
        "std_values.rename(columns={'B1': '442', 'B2': '492', 'B3': '559', 'B4': '665', 'B5': '704', 'B6': '740', 'B7': '780', 'B8': '833', 'B8A': '864', 'B9': '944', 'B11': '1612', 'B12': '2190'}, inplace=True)\n",
        "\n",
        "# transpose data\n",
        "mean_values = mean_values.transpose()\n",
        "mean_values = mean_values.reset_index(drop=False)\n",
        "\n",
        "# grab the first row for the header\n",
        "new_header = mean_values.iloc[0]\n",
        "\n",
        "# take the data less the header row\n",
        "mean_values = mean_values[1:]\n",
        "\n",
        "# set the header row as the df header\n",
        "mean_values.columns = new_header\n",
        "\n",
        "# transpose data\n",
        "std_values = std_values.transpose()\n",
        "std_values = std_values.reset_index(drop=False)\n",
        "\n",
        "# grab the first row for the header\n",
        "new_header2 = std_values.iloc[0]\n",
        "new_header2\n",
        "\n",
        "# take the data less the header row\n",
        "std_values = std_values[1:]\n",
        "std_values.columns = new_header2\n",
        "\n",
        "# calculate mean values for Water label\n",
        "mean_values.W.values\n",
        "mean_w = np.array(mean_values.W.values, dtype=float)\n",
        "\n",
        "# calculate standard deviation values for Water label\n",
        "std_values.W.values\n",
        "st_w = np.array(std_values.W.values, dtype=float)\n",
        "\n",
        "# calculate mean values for Water hyacinth label\n",
        "mean_values.WH.values\n",
        "mean_wh = np.array(mean_values.WH.values, dtype=float)\n",
        "\n",
        "# calculate standard deviation values for Water hyacinth label\n",
        "std_values.WH.values\n",
        "st_wh = np.array(std_values.WH.values, dtype=float)\n",
        "\n",
        "#Copy mean values\n",
        "test1 = mean_values.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622cc6a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kL4OueXFCl-",
        "outputId": "a129521e-639b-4651-f80b-d06662d673e3"
      },
      "outputs": [],
      "source": [
        "# create figure visualizing the mean spectral signatures of water and water hyacinth labeled pixels\n",
        "# set figure subplots & size\n",
        "fig, axes = plt.subplots(ncols = 2, figsize=(14,4))\n",
        "axes[0].plot(test1.Label, test1.WH, c = 'teal', linewidth = 1, markerfacecolor = 'teal', marker = 'x',markersize = 6)\n",
        "axes[0].plot(test1.Label, test1.W, c = 'navy', linewidth = 1, markerfacecolor = 'navy', marker = 'x',markersize = 6)\n",
        "axes[0].legend(['S2 water hyacinth', 'S2 water'], ncol=1, fontsize = 10)\n",
        "axes[0].set_xlabel(\"Wavelengths [nm]\", fontsize = 12)\n",
        "axes[0].set_ylabel(\"Reflectance\", fontsize = 12)\n",
        "axes[0].set_ylim(0,0.4)\n",
        "axes[1].set_visible(False)\n",
        "\n",
        "# save figure to specified folder and plot figure\n",
        "new_dir_path = './drive/My Drive/SentinelModel/figures'\n",
        "os.makedirs(new_dir_path, exist_ok=True)  # The `exist_ok=True` argument ensures it doesn't throw an error if the directory already exists\n",
        "\n",
        "\n",
        "plt.savefig('./drive/My Drive/SentinelModel/figures/spectral_signatures.png', bbox_inches = \"tight\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ab25feb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-I-GAsSl-4X",
        "outputId": "1393c658-95e1-4ff3-e62b-0845d22351bb"
      },
      "outputs": [],
      "source": [
        "# create figure visualizing the spectral signatures of water and water hyacinth labeled pixels\n",
        "# set figure subplots & size\n",
        "f, axes = plt.subplots(1, 2)\n",
        "sns.set_style(\"ticks\")\n",
        "\n",
        "#create your own color array\n",
        "my_colors = [\"cornflowerblue\", \"teal\"]\n",
        "my_colors1 = [\"cornflowerblue\", \"teal\"]\n",
        "\n",
        "# add color array to set_palette\n",
        "sns.set_palette(my_colors)\n",
        "\n",
        "# add boxplot of ndvi data\n",
        "sns.boxplot(x=\"Label\", y=\"ndvi\",\n",
        "data=testing_data, width=0.3, boxprops=dict(alpha=.65), medianprops=dict(color=\"red\", alpha=1), ax=axes[0])\n",
        "\n",
        "# add stripplot of ndvi data\n",
        "sns.stripplot(x=\"Label\", y=\"ndvi\",\n",
        "data=testing_data, alpha=0.3, palette = my_colors1, ax=axes[0])\n",
        "\n",
        "# set ndvi labels\n",
        "axes[0].set(xticklabels=[\"Water\", \"Water hyacinth's\"])\n",
        "axes[0].set_title('NDVI')\n",
        "axes[0].set_ylabel('NDVI')\n",
        "axes[0].set_xlabel('')\n",
        "\n",
        "# add boxplot of fai data\n",
        "ax = sns.boxplot(x=\"Label\", y=\"fai\",\n",
        "data=testing_data, width=0.3, boxprops=dict(alpha=.65), medianprops=dict(color=\"red\", alpha=1), ax=axes[1])\n",
        "\n",
        "# add stripplot of fai data\n",
        "ax = sns.stripplot(x=\"Label\", y=\"fai\",\n",
        "data=testing_data, alpha=0.2, palette = my_colors1, ax=axes[1])\n",
        "\n",
        "# set fai labels\n",
        "axes[1].set(xticklabels=[\"Water\", \"Water hyacinth's\"])\n",
        "axes[1].set_title('FAI')\n",
        "axes[1].set_ylabel('FAI')\n",
        "axes[1].set_xlabel('')\n",
        "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.8)\n",
        "\n",
        "# save figure to specified folder and plot figure\n",
        "plt.savefig('./drive/My Drive/SentinelModel/figures/spectral_signatures_fai.png', bbox_inches = \"tight\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fe2e8b",
      "metadata": {
        "id": "3EI63KQzAJkN"
      },
      "source": [
        "## 5. Define area of interest\n",
        "Load shapefile and create bounding boxes for parallel computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9990d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD5Bize0nlbu",
        "outputId": "f032585c-b13f-4966-8d53-04f531e1d1a0"
      },
      "outputs": [],
      "source": [
        "# load geojson file, file can be found on github page\n",
        "river = GeoDataFrame.from_file(os.path.join('./drive/My Drive/SentinelModel/input_data/CPRaoi_V01.json'))\n",
        "\n",
        "# convert CRS to UTM_47N, based on our area of interest\n",
        "river = river.to_crs(crs=CRS.UTM_47N.pyproj_crs())\n",
        "\n",
        "# get the country's shape in polygon format & create a bounding box\n",
        "river_shape = river.geometry.tolist()[-1]\n",
        "minx, miny, maxx, maxy = river_shape.bounds\n",
        "river_bbox = BBox([minx, miny, maxx, maxy], crs=CRS.UTM_47N.pyproj_crs())\n",
        "river_bbox = river_bbox.geometry - river_shape\n",
        "print(river)\n",
        "# split river into multiple smaller bounding boxes\n",
        "bbox_splitter = BBoxSplitter([river_shape], CRS.UTM_47N.pyproj_crs(), (3,4))\n",
        "bbox_list = np.array(bbox_splitter.get_bbox_list())\n",
        "info_list = np.array(bbox_splitter.get_info_list())\n",
        "\n",
        "# create patchIDs that can be used for parallel computing\n",
        "patchIDs = []\n",
        "for idx, [bbox, info] in enumerate(zip(bbox_list, info_list)):\n",
        "    patchIDs.append(idx)\n",
        "\n",
        "# prepare info of selected EOPatches\n",
        "geometry = [Polygon1(bbox.get_polygon()) for bbox in bbox_list[patchIDs]]\n",
        "idxs_x = [info['index_x'] for info in info_list[patchIDs]]\n",
        "idxs_y = [info['index_y'] for info in info_list[patchIDs]]\n",
        "\n",
        "# create geodataframe using the idxs_x and idxs_y\n",
        "gdf = gpd.GeoDataFrame({'index_x': idxs_x, 'index_y': idxs_y},\n",
        "                       crs=CRS.UTM_47N.pyproj_crs(),\n",
        "                       geometry=geometry)\n",
        "\n",
        "# create polygon and set fontdict that can be used for figure\n",
        "poly = gdf['geometry'][0]\n",
        "x1, y1, x2, y2 = poly.bounds\n",
        "aspect_ratio = (y1 - y2) / (x1 - x2)\n",
        "fontdict = {'family': 'monospace', 'weight': 'normal', 'size': 14}\n",
        "\n",
        "# if bboxes have all same size, estimate offset\n",
        "xl, yl, xu, yu = gdf.geometry[0].bounds\n",
        "xoff, yoff = (xu-xl)/3, (yu-yl)/3\n",
        "\n",
        "# figure with bboxes and river shapefile plotted\n",
        "fig, ax = plt.subplots(figsize=(20,20))\n",
        "gdf.plot(ax=ax, facecolor='w', edgecolor='r', alpha=0.5, linewidth=2)\n",
        "river.plot(ax=ax, facecolor='w', edgecolor='b', alpha=0.5, linewidth=2.5)\n",
        "ax.set_title('River tiled in a 3 by 4 grid');\n",
        "\n",
        "# add annotiation text figure\n",
        "for idx in gdf.index:\n",
        "    eop_name = '{0}x{1}'.format(gdf.index_x[idx], gdf.index_y[idx])\n",
        "    centroid, = list(gdf.geometry[idx].centroid.coords)\n",
        "    ax.text(centroid[0]-xoff, centroid[1]+yoff, '{}'.format(idx), fontdict=fontdict)\n",
        "\n",
        "# save figure to specified folder and plot figure\n",
        "plt.savefig('./drive/My Drive/SentinelModel/figures/area_of_interest_bboxes.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e490121c",
      "metadata": {
        "id": "s46RdoX_CHtH"
      },
      "outputs": [],
      "source": [
        "# Export river shape\n",
        "river.to_file('./drive/My Drive/SentinelModel/input_data/river_shape.geojson', driver='GeoJSON')\n",
        "\n",
        "# Export river grid tiles (gdf)\n",
        "gdf.to_file('./drive/My Drive/SentinelModel/input_data/river_grid_tiles.geojson', driver='GeoJSON')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c60cbc",
      "metadata": {
        "id": "tUfg1ZpOAhEk"
      },
      "source": [
        "## 6. Create EO workflow to:\n",
        "*   Download data\n",
        "*   Create a cloud mask\n",
        "*   Create a river mask\n",
        "*   Add FAI, NDVI\n",
        "*   Add spatial temporal features\n",
        "*   Perform Naive bayes classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de55937b",
      "metadata": {
        "id": "Jy3-DggHpmfD"
      },
      "source": [
        "### Set time interval and define nominal water"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d020e3b4",
      "metadata": {
        "id": "c9Fg_f0MIba1"
      },
      "outputs": [],
      "source": [
        "#adapt time_interval according to period of interest\n",
        "time_interval = ['2024-06-01', '2025-06-01'] # time interval for the SH request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df62a165",
      "metadata": {
        "id": "alUA5tMElPP5"
      },
      "outputs": [],
      "source": [
        "# create nominal water mask that can be used to extract pixel values that are actually located within the river boundaries\n",
        "add_nominal_water = VectorToRasterTask(river, (FeatureType.MASK_TIMELESS, 'NOMINAL_WATER'), values=1,\n",
        "                                   raster_shape=(FeatureType.MASK, 'IS_DATA'), raster_dtype=np.uint8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "087e4193",
      "metadata": {
        "id": "pG6whlO-Bbm9"
      },
      "source": [
        "### 6.1 EO Tasks to compute 'VALIDE_DATA' based on cloud masks and count the valid observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe186e2",
      "metadata": {
        "collapsed": true,
        "id": "ZqIMiimud_rd"
      },
      "outputs": [],
      "source": [
        "class SentinelHubValidData(EOTask):\n",
        "    \"\"\"\n",
        "    Combine Sen2Cor's classification map with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
        "    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']\n",
        "    \"\"\"\n",
        "    def __call__(self, eopatch):\n",
        "        return np.logical_and(eopatch.mask['IS_DATA'].astype(np.bool),\n",
        "                              np.logical_not(eopatch.mask['CLM'].astype(np.bool)))\n",
        "\n",
        "class CountValid(EOTask):\n",
        "    \"\"\"\n",
        "    The task counts number of valid observations in time-series and stores the results in the timeless mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, count_what, feature_name):\n",
        "        self.what = count_what\n",
        "        self.name = feature_name\n",
        "\n",
        "    def execute(self, eopatch):\n",
        "\n",
        "      # Count non-zero values across the time dimension\n",
        "        valid_counts = np.count_nonzero(eopatch.mask[self.what], axis=0)\n",
        "\n",
        "        # Add the result as a new timeless mask feature\n",
        "        add_feature_task = AddFeatureTask((FeatureType.MASK_TIMELESS, self.name))\n",
        "        eopatch = add_feature_task.execute(eopatch, valid_counts)\n",
        "\n",
        "        return eopatch\n",
        "\n",
        "class SentinelHubValidDataTask(EOTask):\n",
        "    \"\"\"\n",
        "    Combine Sen2Cor's classification map with `IS_DATA` to define a `VALID_DATA_SH` mask\n",
        "    The SentinelHub's cloud mask is asumed to be found in eopatch.mask['CLM']\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_feature):\n",
        "        self.output_feature = output_feature\n",
        "\n",
        "    def execute(self, eopatch):\n",
        "        eopatch[self.output_feature] = eopatch.mask[\"IS_DATA\"].astype(bool) & (~eopatch.mask[\"CLM\"].astype(bool))\n",
        "        return eopatch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "159f59bb",
      "metadata": {
        "id": "qJL4HuLJBgx6"
      },
      "source": [
        "### 6.2 Create workflow and required classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321e4119",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_EdkmiBAUPf",
        "outputId": "25f61fdb-1a63-4287-bb05-30bedad480bd"
      },
      "outputs": [],
      "source": [
        "#add a request for B(B02), G(B03), R(B04), NIR (B08), SWIR1(B11), SWIR2(B12)\n",
        "# from default layer 'ALL_BANDS' at 10m resolution\n",
        "# Here we also do a simple filter of cloudy scenes. A detailed cloud cover\n",
        "# detection is performed in the next step\n",
        "# use sentinelhubinputtask function that retrieves the specified data for our area of interest\n",
        "custom_script = ['B02', 'B03', 'B04', 'B06', 'B08', 'B11', 'B12']\n",
        "add_data = SentinelHubInputTask(\n",
        "    bands_feature=(FeatureType.DATA, 'BANDS'),\n",
        "    bands = custom_script,\n",
        "    resolution=10,\n",
        "    maxcc=0.8,\n",
        "    data_collection=DataCollection.SENTINEL2_L2A, #change DataSource to DataCollection\n",
        "    additional_data=[(FeatureType.MASK, 'dataMask', 'IS_DATA'),\n",
        "                     (FeatureType.MASK, 'CLM'),\n",
        "                     (FeatureType.DATA, 'CLP'),\n",
        "                     (FeatureType.MASK, 'SCL')])\n",
        "\n",
        "# class that defines if valid data fraction is above the specified threshold\n",
        "class ValidDataFractionPredicate:\n",
        "    \"\"\" Predicate that defines if a frame from EOPatch's time-series is valid or not. Frame is valid, if the\n",
        "    valid data fraction is above the specified threshold.\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def __call__(self, array):\n",
        "        coverage = np.sum(array.astype(np.uint8)) / np.prod(array.shape)\n",
        "        return coverage > self.threshold\n",
        "\n",
        "# class that calculates ndvi on a pixel level\n",
        "class NormalizedDifferenceIndex(EOTask):\n",
        "\n",
        "    def __init__(self, feature_name, band_a, band_b):\n",
        "        self.feature_name = feature_name\n",
        "        self.band_a_fetaure_name = band_a.split('/')[0]\n",
        "        self.band_b_fetaure_name = band_b.split('/')[0]\n",
        "        self.band_a_fetaure_idx = int(band_a.split('/')[-1])\n",
        "        self.band_b_fetaure_idx = int(band_b.split('/')[-1])\n",
        "\n",
        "    def execute(self, eopatch):\n",
        "        band_a = eopatch.data[self.band_a_fetaure_name][..., self.band_a_fetaure_idx]\n",
        "        band_b = eopatch.data[self.band_b_fetaure_name][..., self.band_b_fetaure_idx]\n",
        "\n",
        "        ndvi = (band_a - band_b) / (band_a  + band_b)\n",
        "\n",
        "        #eopatch.add_feature(FeatureType.DATA, self.feature_name, ndvi[..., np.newaxis])\n",
        "        #eopatch[FeatureType.DATA][self.feature_name] = ndvi[..., np.newaxis]\n",
        "        add_feature_task = AddFeatureTask((FeatureType.DATA, self.feature_name))\n",
        "        eopatch = add_feature_task.execute(eopatch, ndvi[..., np.newaxis])\n",
        "\n",
        "        return eopatch\n",
        "\n",
        "# class that calculates fai on a pixel level\n",
        "class FloatingAlgaeIndex(EOTask):\n",
        "\n",
        "    def __init__(self, feature_name, band_a, band_b, band_c, band_d, band_e, band_f, band_g):\n",
        "        self.feature_name = feature_name\n",
        "        self.band_a_fetaure_name = band_a.split('/')[0]\n",
        "        self.band_b_fetaure_name = band_b.split('/')[0]\n",
        "        self.band_c_fetaure_name = band_c.split('/')[0]\n",
        "        self.band_d_fetaure_name = band_d.split('/')[0]\n",
        "        self.band_e_fetaure_name = band_e.split('/')[0]\n",
        "        self.band_f_fetaure_name = band_f.split('/')[0]\n",
        "        self.band_g_fetaure_name = band_g.split('/')[0]\n",
        "        self.band_a_fetaure_idx = int(band_a.split('/')[-1])\n",
        "        self.band_b_fetaure_idx = int(band_b.split('/')[-1])\n",
        "        self.band_c_fetaure_idx = int(band_c.split('/')[-1])\n",
        "        self.band_d_fetaure_idx = int(band_d.split('/')[-1])\n",
        "        self.band_e_fetaure_idx = int(band_e.split('/')[-1])\n",
        "        self.band_f_fetaure_idx = int(band_f.split('/')[-1])\n",
        "        self.band_g_fetaure_idx = int(band_g.split('/')[-1])\n",
        "\n",
        "    def execute(self, eopatch):\n",
        "        band_a = eopatch.data[self.band_a_fetaure_name][..., self.band_a_fetaure_idx]\n",
        "        band_b = eopatch.data[self.band_b_fetaure_name][..., self.band_b_fetaure_idx]\n",
        "        band_c = eopatch.data[self.band_c_fetaure_name][..., self.band_c_fetaure_idx]\n",
        "        band_d = eopatch.data[self.band_d_fetaure_name][..., self.band_d_fetaure_idx]\n",
        "        band_e = eopatch.data[self.band_e_fetaure_name][..., self.band_e_fetaure_idx]\n",
        "        band_f = eopatch.data[self.band_f_fetaure_name][..., self.band_f_fetaure_idx]\n",
        "        band_g = eopatch.data[self.band_g_fetaure_name][..., self.band_g_fetaure_idx]\n",
        "\n",
        "\n",
        "        fai = band_e - (((band_c * (842-1610)) + (band_g * (665-842)))/(665-1614))\n",
        "\n",
        "        #eopatch.add_feature(FeatureType.DATA, self.feature_name, fai[..., np.newaxis])\n",
        "        #eopatch[FeatureType.DATA][self.feature_name] = fai[..., np.newaxis]\n",
        "        add_feature_task = AddFeatureTask((FeatureType.DATA, self.feature_name))\n",
        "        eopatch = add_feature_task.execute(eopatch, fai[..., np.newaxis])\n",
        "\n",
        "\n",
        "        return eopatch\n",
        "\n",
        "# class that uses all the required input values on a pixel level and classifies WH & W using the trained Naive Bayes model\n",
        "class WaterhyacinthClassification(EOTask):\n",
        "\n",
        "    def __init__(self, feature_name, band_a, band_b, band_c, band_d, band_e, band_f, band_g):\n",
        "        self.feature_name = feature_name\n",
        "        self.band_a_fetaure_name = band_a.split('/')[0]\n",
        "        self.band_b_fetaure_name = band_b.split('/')[0]\n",
        "        self.band_c_fetaure_name = band_c.split('/')[0]\n",
        "        self.band_d_fetaure_name = band_d.split('/')[0]\n",
        "        self.band_e_fetaure_name = band_e.split('/')[0]\n",
        "        self.band_f_fetaure_name = band_f.split('/')[0]\n",
        "        self.band_g_fetaure_name = band_g.split('/')[0]\n",
        "        self.band_a_fetaure_idx = int(band_a.split('/')[-1])\n",
        "        self.band_b_fetaure_idx = int(band_b.split('/')[-1])\n",
        "        self.band_c_fetaure_idx = int(band_c.split('/')[-1])\n",
        "        self.band_d_fetaure_idx = int(band_d.split('/')[-1])\n",
        "        self.band_e_fetaure_idx = int(band_e.split('/')[-1])\n",
        "        self.band_f_fetaure_idx = int(band_f.split('/')[-1])\n",
        "        self.band_g_fetaure_idx = int(band_g.split('/')[-1])\n",
        "\n",
        "    def execute(self, eopatch):\n",
        "        band_a = eopatch.data[self.band_a_fetaure_name][..., self.band_a_fetaure_idx]\n",
        "        band_b = eopatch.data[self.band_b_fetaure_name][..., self.band_b_fetaure_idx]\n",
        "        band_c = eopatch.data[self.band_c_fetaure_name][..., self.band_c_fetaure_idx]\n",
        "        band_d = eopatch.data[self.band_d_fetaure_name][..., self.band_d_fetaure_idx]\n",
        "        band_e = eopatch.data[self.band_e_fetaure_name][..., self.band_e_fetaure_idx]\n",
        "        band_f = eopatch.data[self.band_f_fetaure_name][..., self.band_f_fetaure_idx]\n",
        "        band_g = eopatch.data[self.band_g_fetaure_name][..., self.band_g_fetaure_idx]\n",
        "\n",
        "        fai = eopatch.data['FAI']\n",
        "        ndvi = eopatch.data['NDVI']\n",
        "        band_c = eopatch.data['BANDS'][..., [2]]\n",
        "        band_e = eopatch.data['BANDS'][..., [4]]\n",
        "        band_f = eopatch.data['BANDS'][..., [6]]\n",
        "\n",
        "        img = np.concatenate((ndvi, fai, band_c, band_e, band_f),axis=-1)\n",
        "        img = np.nan_to_num(img)\n",
        "\n",
        "        class_prediction = class_model.predict(img.reshape(-1, 5))\n",
        "\n",
        "        # reshape our classification map back into a 2D matrix so we can visualize it\n",
        "        class_prediction = class_prediction.reshape(img[:, :, :, 0].shape)\n",
        "\n",
        "        #eopatch.add_feature(FeatureType.DATA, self.feature_name, class_prediction[..., np.newaxis])\n",
        "        #eopatch[FeatureType.DATA][self.feature_name] = class_prediction[..., np.newaxis]\n",
        "        add_feature_task = AddFeatureTask((FeatureType.DATA, self.feature_name))\n",
        "        eopatch = add_feature_task.execute(eopatch, class_prediction[..., np.newaxis])\n",
        "\n",
        "        return eopatch\n",
        "\n",
        "# tasks for calculating new features\n",
        "ndvi = NormalizedDifferenceIndex('NDVI', 'BANDS/4', 'BANDS/2')\n",
        "ndwi = NormalizedDifferenceIndex('NDWI', 'BANDS/1', 'BANDS/3')\n",
        "plastic = NormalizedDifferenceIndex('PLASTIC', 'BANDS/4', 'BANDS/0')\n",
        "fai = FloatingAlgaeIndex('FAI', 'BANDS/0', 'BANDS/1', 'BANDS/2', 'BANDS/3', 'BANDS/4', 'BANDS/5', 'BANDS/6')\n",
        "whc = WaterhyacinthClassification('WHC', 'BANDS/0', 'BANDS/1', 'BANDS/2', 'BANDS/3', 'BANDS/4', 'BANDS/5', 'BANDS/6')\n",
        "\n",
        "# calculate ratio of coverage\n",
        "def calculate_coverage(array):\n",
        "    return 1.0 - np.count_nonzero(array) / np.size(array)\n",
        "\n",
        "# add validdatacoverage as layer within the EOpatch\n",
        "class AddValidDataCoverage(EOTask):\n",
        "\n",
        "     def execute(self, eopatch):\n",
        "      valid_data = eopatch.mask['IS_VALID']\n",
        "      time, height, width, channels = valid_data.shape\n",
        "\n",
        "      coverage = np.apply_along_axis(calculate_coverage, 1,\n",
        "                                       valid_data.reshape((time, height * width * channels)))\n",
        "\n",
        "      # eopatch.add_feature(FeatureType.SCALAR, 'COVERAGE', coverage[:, np.newaxis])\n",
        "      add_feature_task = AddFeatureTask((FeatureType.SCALAR, 'COVERAGE'))\n",
        "      eopatch = add_feature_task.execute(eopatch, coverage[:, np.newaxis])\n",
        "      return eopatch\n",
        "\n",
        "add_coverage = AddValidDataCoverage()\n",
        "\n",
        "# VALIDITY MASK\n",
        "# Validate pixels using SentinelHub's cloud detection mask and region of acquisition\n",
        "add_sh_validmask = SentinelHubValidDataTask((FeatureType.MASK, \"IS_VALID\"))\n",
        "\n",
        "# TASK FOR COUNTING VALID PIXELS\n",
        "# count number of valid observations per pixel using valid data mask\n",
        "count_val_sh = CountValid('IS_VALID','VALID_COUNT')\n",
        "\n",
        "# TASK FOR FILTERING OUT TOO CLOUDY SCENES\n",
        "# keep frames with > 80 % valid coverage\n",
        "valid_data_predicate = ValidDataFractionPredicate(0.01)\n",
        "filter_task = SimpleFilterTask((FeatureType.MASK, 'IS_VALID'), valid_data_predicate)\n",
        "\n",
        "# TASK FOR SAVING TO OUTPUT (if needed & only do this when there is no data yet)\n",
        "# save output to specified folder and plot figure\n",
        "path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/'\n",
        "if not os.path.isdir(path_out):\n",
        "    os.makedirs(path_out)\n",
        "\n",
        "# Save task with the new OneDrive path\n",
        "save = SaveTask(path_out, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f699ac90",
      "metadata": {
        "id": "A67w-IntB5_-"
      },
      "source": [
        "### 6.3 Define the workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffa40d93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "h6LUiEyd9TaX",
        "outputId": "3b381e9c-e75e-4e19-80e7-3377f2a07261"
      },
      "outputs": [],
      "source": [
        "#Specify nodes\n",
        "node1 = EONode(add_data, name='SentinelHubInputTask')\n",
        "node2 = EONode(add_nominal_water, inputs=[node1])\n",
        "node3 = EONode(fai, inputs=[node2])\n",
        "node4 = EONode(ndvi, inputs=[node3])\n",
        "node5 = EONode(whc, inputs=[node4])\n",
        "node6 = EONode(plastic, inputs=[node5])\n",
        "node7 = EONode(add_sh_validmask, inputs=[node6])\n",
        "node8 = EONode(add_coverage, inputs=[node7])\n",
        "node9 = EONode(filter_task, inputs=[node8])\n",
        "node10 = EONode(count_val_sh, inputs=[node9])\n",
        "node11 = EONode(save, inputs=[node10])\n",
        "\n",
        "workflow = EOWorkflow([node1, node2, node3, node4, node5, node6, node7, node8, node9, node10, node11])\n",
        "workflow.dependency_graph('graph.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608c5f18",
      "metadata": {
        "id": "a17Hnm4zB9db"
      },
      "source": [
        "### 6.4 Execute the workflow for each patch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a903a527",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "IN6f7vCs7lqt",
        "outputId": "151bc7e5-43ad-4382-ff37-5e8574e95f39"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Execute the workflow\n",
        "# define additional parameters of the workflow\n",
        "execution_args = []\n",
        "for idx, bbox in enumerate(bbox_list[patchIDs]):\n",
        "    execution_args.append({\n",
        "        node1:{'bbox': bbox, 'time_interval': time_interval},\n",
        "        node11: {'eopatch_folder': 'eopatch_{}'.format(idx)}\n",
        "    })\n",
        "\n",
        "executor = EOExecutor(workflow, execution_args, save_logs=True)\n",
        "executor.run(workers=3, multiprocess=True)\n",
        "\n",
        "executor.make_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa98da67",
      "metadata": {
        "id": "1uqMHw4qCDdc"
      },
      "source": [
        "### 6.5 Loop to create a list with dates in which each patch contains the same dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5735bd42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFcX4NAzeiKv",
        "outputId": "16b465b9-ecd4-4c14-b851-8fe3309e3c12"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Define EOPatch paths for uppermost area\n",
        "eop_locations = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_5',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_8',\n",
        "]\n",
        "\n",
        "# Load EOPatches (not lazy)\n",
        "eopatches = []\n",
        "for loc in eop_locations:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=False)\n",
        "        eopatches.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load EOPatch at {loc}: {e}\")\n",
        "\n",
        "# Collect all unique timestamps (no filtering)\n",
        "dt_list = set()\n",
        "for eop in eopatches:\n",
        "    dt_list.update(eop.timestamps)\n",
        "\n",
        "dt_list = sorted(list(dt_list))\n",
        "date_list_1 = [dt.strftime('%m/%d/%Y') for dt in dt_list]\n",
        "\n",
        "\n",
        "# Output result\n",
        "print(\"All unique timestamps across upper area patches:\", date_list_1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5827b482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WPKGYdqy1In",
        "outputId": "20905ddf-fb85-46d7-d802-d4583a15ea8e"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Define EOPatch paths for middle-upper area\n",
        "eop_locations = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_4',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_7'\n",
        "]\n",
        "\n",
        "# Load EOPatches (not lazy)\n",
        "eopatches = []\n",
        "for loc in eop_locations:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=False)\n",
        "        eopatches.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load EOPatch at {loc}: {e}\")\n",
        "\n",
        "# Collect all unique timestamps (no filtering)\n",
        "dt_list = set()\n",
        "for eop in eopatches:\n",
        "    dt_list.update(eop.timestamps)\n",
        "\n",
        "dt_list = sorted(list(dt_list))\n",
        "date_list_2 = [dt.strftime('%m/%d/%Y') for dt in dt_list]\n",
        "\n",
        "\n",
        "# Output result\n",
        "print(\"All unique timestamps across upper area patches:\", date_list_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ad826e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivmUI25Ce6QU",
        "outputId": "ef130e01-8d97-4bd1-c8c0-7565c9761ef0"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Define paths for middle-lower area EOPatches\n",
        "eop_locations = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_1',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_3'\n",
        "]\n",
        "\n",
        "# Load EOPatches (not lazy!)\n",
        "eopatches = []\n",
        "for loc in eop_locations:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=False)\n",
        "        eopatches.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load EOPatch at {loc}: {e}\")\n",
        "\n",
        "# Collect all unique timestamps across patches\n",
        "dt_list = set()\n",
        "for eop in eopatches:\n",
        "    dt_list.update(eop.timestamps)\n",
        "\n",
        "dt_list = sorted(list(dt_list))\n",
        "date_list_3 = [dt.strftime('%m/%d/%Y') for dt in dt_list]\n",
        "\n",
        "\n",
        "# Output result\n",
        "print(\"All unique timestamps across middle area patches:\", date_list_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e899e350",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_cqY1GSfCC_",
        "outputId": "21b0f167-744f-4722-a2b2-99f81b2e5cb5"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define paths for lower area EOPatches\n",
        "eop_locations = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_0',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_2',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatch_6'\n",
        "]\n",
        "\n",
        "# Fully load available EOPatches\n",
        "eopatches = []\n",
        "for loc in eop_locations:\n",
        "    if os.path.exists(loc):\n",
        "        try:\n",
        "            eop = EOPatch.load(loc, lazy_loading=False)  # âœ… No lazy loading\n",
        "            eopatches.append(eop)\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load EOPatch at {loc}: {e}\")\n",
        "    else:\n",
        "        print(f\"EOPatch directory does not exist: {loc}\")\n",
        "\n",
        "# Collect all unique timestamps from loaded patches\n",
        "dt_list = set()\n",
        "for eop in eopatches:\n",
        "    dt_list.update(eop.timestamps)\n",
        "\n",
        "dt_list = sorted(list(dt_list))\n",
        "date_list_4 = [dt.strftime('%m/%d/%Y') for dt in dt_list]\n",
        "\n",
        "\n",
        "# Print result\n",
        "print(\"All unique timestamps across lower area patches:\", date_list_4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30a11a64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17jmXixsCPQJ",
        "outputId": "687612e6-5eb6-4393-e98d-b42c08b4423e"
      },
      "outputs": [],
      "source": [
        "print(date_list_1)\n",
        "print(date_list_2)\n",
        "print(date_list_3)\n",
        "print(date_list_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85322cc7",
      "metadata": {
        "id": "SQYoDinDCXlo"
      },
      "source": [
        "### 6.6 Cloud masking system to create masks based on 10m resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a08101",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzVBP-fCFEoC",
        "outputId": "d47effda-cc82-46ec-944c-1a323a5d611b"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOTask, EOPatch, LoadTask, SaveTask, FeatureType, OverwritePermission\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "\n",
        "# Set path to output folder and create load/save tasks\n",
        "path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/'\n",
        "path_out_sampled = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled'\n",
        "\n",
        "if not os.path.isdir(path_out_sampled):\n",
        "    os.makedirs(path_out_sampled)\n",
        "\n",
        "load = LoadTask(path_out, lazy_loading=True)\n",
        "save = SaveTask(path_out_sampled, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
        "\n",
        "# create a list of dates that can be used as filter to exclude images that do not overlap area of interest completely\n",
        "def dt_list_filter(date, date_list, tolerance):\n",
        "    return np.min(np.abs(date - np.array(date_list))) <= tolerance\n",
        "\n",
        "# Cloud masking task\n",
        "class cloud_mask(EOTask):\n",
        "    def __init__(self, feature_name):\n",
        "        self.feature_name = feature_name\n",
        "\n",
        "    def execute(self, eopatch):\n",
        "        clm = eopatch.mask[\"CLM\"]\n",
        "        scl = eopatch.mask[\"SCL\"]\n",
        "\n",
        "        valid_clm = clm == 1\n",
        "        valid_scl = (scl == 4) | (scl == 5) | (scl == 6)\n",
        "\n",
        "        valid_scl_resized = np.zeros_like(valid_clm, dtype=bool)\n",
        "        for t in range(valid_clm.shape[0]):\n",
        "            valid_scl_resized[t, ..., 0] = resize(\n",
        "                valid_scl[t, ..., 0],\n",
        "                (valid_clm.shape[1], valid_clm.shape[2]),\n",
        "                order=0,\n",
        "                preserve_range=True\n",
        "            ).astype(bool)\n",
        "\n",
        "        valid_mask = valid_clm | valid_scl_resized\n",
        "        eopatch[FeatureType.MASK][self.feature_name] = valid_mask\n",
        "        return eopatch\n",
        "\n",
        "# Create instance of the mask task\n",
        "cloud_mask2 = cloud_mask('VALID_MASK')\n",
        "\n",
        "# # Loop over EOPatches manually\n",
        "# for i in range(9):\n",
        "#     patch_name = f'eopatch_{i}'\n",
        "#     try:\n",
        "#         # Load patch\n",
        "#         eopatch = load.execute(eopatch_folder=patch_name)\n",
        "\n",
        "#         # Apply cloud masking\n",
        "#         eopatch = cloud_mask2.execute(eopatch)\n",
        "\n",
        "#         # Save patch\n",
        "#         save.execute(eopatch=eopatch, eopatch_folder=patch_name)\n",
        "\n",
        "#         print(f\"{patch_name} processed successfully.\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"Failed to process {patch_name}: {e}\")\n",
        "\n",
        "# Assumes dt_list is already defined as a list of datetime objects\n",
        "filter_task2 = SimpleFilterTask(\n",
        "    feature='timestamps',\n",
        "    filter_func=partial(dt_list_filter, date_list=dt_list, tolerance=timedelta(days=0))\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f452cc",
      "metadata": {
        "id": "ufNxc2a7Couj"
      },
      "source": [
        "### 6.7 Define the workflow #2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263277dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "HtydBfj1FaS2",
        "outputId": "4453a927-550b-4fad-9069-2f6318faec01"
      },
      "outputs": [],
      "source": [
        "# Define the workflow #2\n",
        "\n",
        "newnode1 = EONode(load, name='load')\n",
        "newnode2 = EONode(cloud_mask2, inputs=[newnode1])\n",
        "newnode3 = EONode(filter_task2, inputs=[newnode2])\n",
        "newnode4 = EONode(save, inputs=[newnode3])\n",
        "\n",
        "workflow2 = EOWorkflow([newnode1, newnode2, newnode3, newnode4])\n",
        "workflow2.dependency_graph('graph.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6d9c22",
      "metadata": {
        "id": "3nhv6Kg4CxFf"
      },
      "source": [
        "### 6.8 Execute the workflow for each patch #2\n",
        "In this workflow we are filtering the images in order to exclude cloudy pixels and images that do not completely cover the area of interest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2d22ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "da-WfjA0FoGa",
        "outputId": "e14e0e9e-32eb-4b3f-ed16-51b9ae6cad7d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "execution_args = []\n",
        "for idx in range(9):\n",
        "    execution_args.append({\n",
        "        newnode1: {'eopatch_folder': f'eopatch_{idx}'},\n",
        "        newnode4: {'eopatch_folder': f'eopatch_{idx}'}\n",
        "    })\n",
        "\n",
        "executor = EOExecutor(workflow2, execution_args, save_logs=True)\n",
        "executor.run(workers=1, multiprocess=False)\n",
        "\n",
        "executor.make_report()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7852e7c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "k4NLYMjJlDkS",
        "outputId": "a4deb0ec-ec4d-4d9e-8318-101494ac2e48"
      },
      "outputs": [],
      "source": [
        "# from eolearn.core import EOPatch, EOTask, LoadTask, SaveTask, FeatureType, OverwritePermission\n",
        "# import os\n",
        "# import numpy as np\n",
        "# from skimage.transform import resize\n",
        "\n",
        "# # === Set your paths ===\n",
        "# path_in = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/'  # original full patches\n",
        "# path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/'  # where new patches will go\n",
        "\n",
        "# # Ensure output directory exists\n",
        "# os.makedirs(path_out, exist_ok=True)\n",
        "\n",
        "# # === Load & Save tasks ===\n",
        "# load = LoadTask(path_in, lazy_loading=True)\n",
        "# save = SaveTask(path_out, overwrite_permission=OverwritePermission.OVERWRITE_PATCH)\n",
        "\n",
        "# # === Optional cloud mask task ===\n",
        "# class CloudMask(EOTask):\n",
        "#     def __init__(self, feature_name):\n",
        "#         self.feature_name = feature_name\n",
        "\n",
        "#     def execute(self, eopatch):\n",
        "#         clm = eopatch.mask['CLM']\n",
        "#         scl = eopatch.mask['SCL']\n",
        "#         valid_clm = clm == 1\n",
        "#         valid_scl = (scl == 4) | (scl == 5) | (scl == 6)\n",
        "\n",
        "#         valid_scl_resized = np.zeros_like(valid_clm, dtype=bool)\n",
        "#         for t in range(valid_clm.shape[0]):\n",
        "#             valid_scl_resized[t, ..., 0] = resize(\n",
        "#                 valid_scl[t, ..., 0],\n",
        "#                 (valid_clm.shape[1], valid_clm.shape[2]),\n",
        "#                 order=0,\n",
        "#                 preserve_range=True\n",
        "#             ).astype(bool)\n",
        "\n",
        "#         valid_mask = valid_clm | valid_scl_resized\n",
        "#         eopatch[FeatureType.MASK][self.feature_name] = valid_mask\n",
        "#         return eopatch\n",
        "\n",
        "# cloud_mask_task = CloudMask('VALID_MASK')\n",
        "\n",
        "# # === Rebuild all 9 patches ===\n",
        "# for i in range(9):\n",
        "#     patch_name = f'eopatch_{i}'\n",
        "#     try:\n",
        "#         print(f\"Processing {patch_name}...\")\n",
        "#         eop = load.execute(eopatch_folder=patch_name)\n",
        "#         eop = cloud_mask_task.execute(eop)  # optional step\n",
        "#         save.execute(eopatch=eop, eopatch_folder=patch_name)\n",
        "#         print(f\"{patch_name} saved âœ…\")\n",
        "#     except Exception as e:\n",
        "#         print(f\"âŒ Failed to process {patch_name}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afba630",
      "metadata": {
        "id": "oSqsu2kE0Z_J"
      },
      "source": [
        "### 6.9 Lazy load eopatches, extract values and insert them into a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf962a3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra0985Wh1LdG",
        "outputId": "dc87734f-392a-4428-d679-e41dd66cdd71"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define base path\n",
        "base_path = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/'\n",
        "eop_locations = [os.path.join(base_path, f\"eopatch_{i}\") for i in range(9)]\n",
        "\n",
        "# Load all valid EOPatches fully\n",
        "eopatches = []\n",
        "for loc in eop_locations:\n",
        "    if os.path.exists(loc):\n",
        "        try:\n",
        "            eop = EOPatch.load(loc, lazy_loading=False)  # âœ… FULL load\n",
        "            eopatches.append(eop)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load {loc}: {e}\")\n",
        "    else:\n",
        "        print(f\"Missing: {loc}\")\n",
        "\n",
        "# Collect and format all unique timestamps\n",
        "all_dates = set()\n",
        "for eop in eopatches:\n",
        "    all_dates.update(eop.timestamps)\n",
        "\n",
        "# Sort and format\n",
        "all_dates = sorted(all_dates)\n",
        "date_list_sampled = [d.strftime('%m/%d/%Y') for d in all_dates]\n",
        "\n",
        "# Output result\n",
        "print(\"All unique timestamps across available EOPatches:\")\n",
        "print(date_list_sampled)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ada28b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx7K4_OfPOpi",
        "outputId": "2be3da92-00a1-4b62-81e0-77ccf08b68fe"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Lazy load upstream area (patches 5, 8, 4, 7)\n",
        "eop_locations_upstream = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_5',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_8',\n",
        "]\n",
        "\n",
        "eopatches_upstream = []\n",
        "for loc in eop_locations_upstream:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=False)\n",
        "        eopatches_upstream.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {loc}: {e}\")\n",
        "\n",
        "# Print timestamps per patch\n",
        "for i, eop in enumerate(eopatches_upstream):\n",
        "    print(f\"EOPatch {i} timestamps: {[ts.strftime('%Y-%m-%d') for ts in eop.timestamps]}\")\n",
        "\n",
        "# Collect all unique timestamps across upstream patches\n",
        "all_dates_upstream = set()\n",
        "for eop in eopatches_upstream:\n",
        "    all_dates_upstream.update(eop.timestamps)\n",
        "\n",
        "# Sort and format\n",
        "all_dates_upstream = sorted(list(all_dates_upstream))\n",
        "date_list_sampled_upstream = [d.strftime('%m/%d/%Y') for d in all_dates_upstream]\n",
        "\n",
        "# Print final date list\n",
        "print(f\"\\nAll unique timestamps in upstream patches: {date_list_sampled_upstream}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9f4557",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2F8HveNzb4h",
        "outputId": "217bce06-0921-4d2a-d984-0d6c4dfeb8fe"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Lazy load upstream area (patches 5, 8, 4, 7)\n",
        "eop_locations_mid_upstream = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_4',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_7'\n",
        "]\n",
        "\n",
        "eopatches_mid_upstream = []\n",
        "for loc in eop_locations_mid_upstream:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=False)\n",
        "        eopatches_mid_upstream.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {loc}: {e}\")\n",
        "\n",
        "# Print timestamps per patch\n",
        "for i, eop in enumerate(eopatches_mid_upstream):\n",
        "    print(f\"EOPatch {i} timestamps: {[ts.strftime('%Y-%m-%d') for ts in eop.timestamps]}\")\n",
        "\n",
        "# Collect all unique timestamps across upstream patches\n",
        "all_dates_mid_upstream = set()\n",
        "for eop in eopatches_mid_upstream:\n",
        "    all_dates_mid_upstream.update(eop.timestamps)\n",
        "\n",
        "# Sort and format\n",
        "all_dates_mid_upstream = sorted(list(all_dates_mid_upstream))\n",
        "date_list_sampled_mid_upstream = [d.strftime('%m/%d/%Y') for d in all_dates_mid_upstream]\n",
        "\n",
        "# Print final date list\n",
        "print(f\"\\nAll unique timestamps in upstream patches: {date_list_sampled_mid_upstream}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca4d163e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m4AEDRxN_FU",
        "outputId": "5224e8d7-2278-4222-f54a-3d3fd4faba4d"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Lazy load midstream area (patches 1, 3)\n",
        "eop_locations_midstream = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_1',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_3',\n",
        "]\n",
        "\n",
        "# Load patches\n",
        "eopatches_midstream = []\n",
        "for loc in eop_locations_midstream:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=False)\n",
        "        eopatches_midstream.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {loc}: {e}\")\n",
        "\n",
        "# Print timestamps per patch\n",
        "for i, eop in enumerate(eopatches_midstream):\n",
        "    print(f\"EOPatch {i} timestamps: {[ts.strftime('%Y-%m-%d') for ts in eop.timestamps]}\")\n",
        "\n",
        "# Collect all unique timestamps from midstream patches\n",
        "all_dates_midstream = set()\n",
        "for eop in eopatches_midstream:\n",
        "    all_dates_midstream.update(eop.timestamps)\n",
        "\n",
        "# Sort and format\n",
        "all_dates_midstream = sorted(all_dates_midstream)\n",
        "date_list_sampled_2 = [d.strftime('%m/%d/%Y') for d in all_dates_midstream]\n",
        "\n",
        "# Print final date list\n",
        "print(f\"\\nAll unique timestamps in midstream patches: {date_list_sampled_2}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f3ff09",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZaDH3388F2s",
        "outputId": "517fd428-b7f5-4b51-91f4-1f2a1b03337e"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import numpy as np\n",
        "\n",
        "# Lazy load downstream area (patches 0, 2, 6)\n",
        "eop_locations_downstream = [\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_0',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_2',\n",
        "    './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled/eopatch_6',\n",
        "]\n",
        "\n",
        "# Load EOPatches\n",
        "eopatches_downstream = []\n",
        "for loc in eop_locations_downstream:\n",
        "    try:\n",
        "        eop = EOPatch.load(loc, lazy_loading=True)\n",
        "        eopatches_downstream.append(eop)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {loc}: {e}\")\n",
        "\n",
        "# Print timestamps per patch\n",
        "for i, eop in enumerate(eopatches_downstream):\n",
        "    print(f\"EOPatch {i} timestamps: {[ts.strftime('%Y-%m-%d') for ts in eop.timestamps]}\")\n",
        "\n",
        "# Collect all unique timestamps from downstream patches\n",
        "all_dates_downstream = set()\n",
        "for eop in eopatches_downstream:\n",
        "    all_dates_downstream.update(eop.timestamps)\n",
        "\n",
        "# Sort and format\n",
        "all_dates_downstream = sorted(all_dates_downstream)\n",
        "date_list_sampled_3 = [d.strftime('%m/%d/%Y') for d in all_dates_downstream]\n",
        "\n",
        "# Print final date list\n",
        "print(f\"\\nAll unique timestamps in downstream patches: {date_list_sampled_3}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eec9fef",
      "metadata": {
        "id": "xEVN-1J8C2Z6"
      },
      "source": [
        "## 7. Visualise the processed imagery"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a97efb3",
      "metadata": {
        "id": "5WgSHpUBqQ0-"
      },
      "source": [
        "### 7.1 correcting the location of patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8be518",
      "metadata": {
        "id": "qflMlVsRPvRk"
      },
      "outputs": [],
      "source": [
        "# for correct location of patches within visusalization below we need to index them, this is done below.\n",
        "#for full extent\n",
        "# indexes = [[ 3,   7,  9],\n",
        "#            [ 2,   6,   ],\n",
        "#            [ 1,   5,   ],\n",
        "#            [ 0,   4,  6]]\n",
        "\n",
        "# places =  [[ 1,   2,  3],\n",
        "#            [ 4,   5,  6],\n",
        "#            [ 7,   8,  9],\n",
        "#            [ 10,  11, 12]]\n",
        "\n",
        "#for lower area\n",
        "indexes = [[None, 5,  8],\n",
        "           [None, 4,  7],\n",
        "           [ 1,   3,   ],\n",
        "           [ 0,   2,  6]]\n",
        "\n",
        "places =  [[ 1,   2,  3],\n",
        "           [ 4,   5,  6],\n",
        "           [ 7,   8,  9],\n",
        "           [ 10,  11, 12]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee78752d",
      "metadata": {
        "id": "eQUvGutkDrZ4"
      },
      "source": [
        "### 7.2 Plot visible spectrum at a particular date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad3d18e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eQNcqVgboz_d",
        "outputId": "0c62518e-fd74-459d-89f8-3c1967f62d11"
      },
      "outputs": [],
      "source": [
        "from eolearn.core import EOPatch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled'\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "k = 0\n",
        "date_idx = 0\n",
        "\n",
        "for i, (idxs, plc) in enumerate(zip(indexes, places)):\n",
        "    for j, (idx, p) in enumerate(zip(idxs, plc)):\n",
        "        if idx is None:\n",
        "            continue  # skip missing patches\n",
        "\n",
        "        k = k + 1\n",
        "        eopatch = EOPatch.load(f'{path_out}/eopatch_{idx}', lazy_loading=False)\n",
        "\n",
        "        ax = plt.subplot(4, 3, p)\n",
        "        plt.imshow(np.clip(eopatch.data['BANDS'][date_idx][..., [2, 1, 0]] * 3.5, 0, 1))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        ax.set_aspect(\"auto\")\n",
        "        del eopatch\n",
        "\n",
        "fig.suptitle('Visible spectrum at single timepoint', y=0.93, size=20)\n",
        "fig.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.savefig('Visible spectrum at single timepoint.jpg')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87152241",
      "metadata": {
        "id": "yhPr1O0hEadw"
      },
      "source": [
        "### 7.3 Take a closer look at a patch (classified Water Hyacinth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798565d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3l1Cue3p_XGJ",
        "outputId": "e7fbbbd5-1fa7-4c2a-e473-f8940fda121e"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from eolearn.core import EOPatch, FeatureType\n",
        "from eolearn.io import ExportToTiffTask\n",
        "from sentinelhub import CRS\n",
        "\n",
        "path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled'\n",
        "\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "date_idx= -1\n",
        "area = []\n",
        "\n",
        "for i,  (idxs, plc) in enumerate(zip(indexes, places)):\n",
        "    for j, (idx, p) in enumerate(zip(idxs, plc)):\n",
        "        if idx is None:\n",
        "            continue  # skip missing patches\n",
        "\n",
        "        k=k+1\n",
        "        eopatch = EOPatch.load(f'{path_out}/eopatch_{idx}', lazy_loading=True)\n",
        "        ax = plt.subplot(4, 3, p)\n",
        "        #mask = eopatch.mask['IS_VALID'][date_idx].astype(bool)\n",
        "        mask1 = eopatch.mask_timeless['NOMINAL_WATER'].astype(bool) # mask of valid pixels\n",
        "        cmi = eopatch.data['WHC'][date_idx]\n",
        "        #cmi[~mask] = np.nan\n",
        "        cmi[~mask1] = np.nan\n",
        "        area = (np.count_nonzero(cmi == 1)) / 100\n",
        "        date = eopatch.timestamp[date_idx]\n",
        "        im = plt.imshow(cmi.squeeze(), cmap=plt.get_cmap('YlGn'))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        ax.set_aspect(\"auto\")\n",
        "        EoPatch_0 = ExportToTiffTask(feature=(FeatureType.DATA, 'WHC'),\n",
        "                         folder=\"./drive/My Drive/SentinelModel/output_folder\",\n",
        "                         crs=CRS.UTM_47N)\n",
        "        EoPatch_0.execute(eopatch, filename=\"s2_\" + str(idx) + \"}.tif\")\n",
        "        del eopatch\n",
        "\n",
        "\n",
        "fig.subplots_adjust(wspace=0, hspace=0)\n",
        "fig.suptitle('Classified Water Hyacinth at: ' + str(date), y=0.93, size=20)\n",
        "print('date of imagery: ', date)\n",
        "cmap=plt.get_cmap('YlGn')\n",
        "my_colors = {\n",
        "    'Water' : 1,\n",
        "    'Water Hyacinth' : 2.0,\n",
        "    }\n",
        "\n",
        "patches = [mpatches.Patch(color=cmap(v), label=k) for k,v in my_colors.items()]\n",
        "\n",
        "plt.legend(handles=patches, prop={'size': 16})\n",
        "plt.savefig('./drive/My Drive/SentinelModel/figures/waterhyacinthcoverage.png', bbox_inches = \"tight\")\n",
        "plt.show()\n",
        "# plot and save figure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33390ab3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zMjM03vQSF_y",
        "outputId": "4db7942f-0f7a-4828-9118-edce35d7e8f9"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from eolearn.core import EOPatch, FeatureType\n",
        "from eolearn.io import ExportToTiffTask\n",
        "from sentinelhub import CRS\n",
        "\n",
        "path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled'\n",
        "\n",
        "fig = plt.figure(figsize=(20, 20))\n",
        "desired_date = datetime(2025, 3, 16).date()\n",
        "area = []\n",
        "k = 0  # Initialize counter\n",
        "\n",
        "for i, (idxs, plc) in enumerate(zip(indexes, places)):\n",
        "    for j, (idx, p) in enumerate(zip(idxs, plc)):\n",
        "        if idx is None:\n",
        "            continue  # skip missing patches\n",
        "\n",
        "        k = k + 1\n",
        "        eopatch = EOPatch.load(f'{path_out}/eopatch_{idx}', lazy_loading=True)\n",
        "\n",
        "        # Find correct date index\n",
        "        timestamps = [ts.date() for ts in eopatch.timestamp]\n",
        "        if desired_date not in timestamps:\n",
        "            print(f\"Date {desired_date} not found in patch {idx}, skipping...\")\n",
        "            continue\n",
        "        date_idx = timestamps.index(desired_date)\n",
        "\n",
        "        ax = plt.subplot(4, 3, p)\n",
        "        mask1 = eopatch.mask_timeless['NOMINAL_WATER'].astype(bool)  # mask of valid pixels\n",
        "        cmi = eopatch.data['WHC'][date_idx]\n",
        "        cmi[~mask1] = np.nan\n",
        "        area = (np.count_nonzero(cmi == 1)) / 100\n",
        "        date = eopatch.timestamp[date_idx]\n",
        "\n",
        "        im = plt.imshow(cmi.squeeze(), cmap=plt.get_cmap('YlGn'))\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        ax.set_aspect(\"auto\")\n",
        "\n",
        "        EoPatch_0 = ExportToTiffTask(\n",
        "            feature=(FeatureType.DATA, 'WHC'),\n",
        "            folder=\"./drive/My Drive/SentinelModel/output_folder\",\n",
        "            crs=CRS.UTM_47N\n",
        "        )\n",
        "        EoPatch_0.execute(eopatch, filename=f\"s2_{idx}.tif\")\n",
        "        del eopatch\n",
        "\n",
        "fig.subplots_adjust(wspace=0, hspace=0)\n",
        "fig.suptitle('Classified Water Hyacinth at: ' + str(date), y=0.93, size=20)\n",
        "print('date of imagery: ', date)\n",
        "\n",
        "cmap = plt.get_cmap('YlGn')\n",
        "my_colors = {\n",
        "    'Water': 1,\n",
        "    'Water Hyacinth': 2.0,\n",
        "}\n",
        "patches = [mpatches.Patch(color=cmap(v), label=k) for k, v in my_colors.items()]\n",
        "\n",
        "plt.legend(handles=patches, prop={'size': 16})\n",
        "plt.savefig('./drive/My Drive/SentinelModel/figures/waterhyacinthcoverage.png', bbox_inches=\"tight\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "639f63ea",
      "metadata": {
        "id": "4keBxTulHZ2t"
      },
      "source": [
        "## 8 Plot results Water Hyacinth classification in hectares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da7eab6",
      "metadata": {
        "id": "OdWFa2W2q9A5"
      },
      "source": [
        "### 8.1 Calculate the area (ha) covered for rivertotal, and river section per date\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb503c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xVcL41wVWQ-q",
        "outputId": "c2930eb6-f4ef-427d-bf04-bfe4c503ba28"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from eolearn.core import EOPatch\n",
        "\n",
        "# --- SETTINGS ---\n",
        "\n",
        "# Folder where your eopatches are stored\n",
        "path_out = './drive/My Drive/SentinelModel/output_folder/eopatches_aoi/eopatches_aoi_sampled'\n",
        "\n",
        "# Patch indices for the total set and for each area:\n",
        "patch_indices_total     = list(range(9))  # example: patches 0 to 9\n",
        "patch_indices_downstream = [0, 2, 6]\n",
        "patch_indices_mid_upstream = [4, 7]\n",
        "patch_indices_upstream   = [5, 8]\n",
        "patch_indices_midstream  = [1,3]\n",
        "\n",
        "# Parameters for processing\n",
        "whc_class_value = 2   # the value in WHC that indicates water hyacinth\n",
        "divisor = 100         # to convert pixel count into area units\n",
        "\n",
        "# --- STEP 1: Determine Common Dates Automatically ---\n",
        "\n",
        "def extract_dates_from_patch(patch_path):\n",
        "    \"\"\"Load an EOPatch and return a set of dates (as date objects) from its timestamps.\"\"\"\n",
        "    eopatch = EOPatch.load(patch_path, lazy_loading=False)\n",
        "    # Extract dates assuming each timestamp is a datetime object\n",
        "    dates = set(ts.date() for ts in eopatch.timestamps)\n",
        "    return dates\n",
        "\n",
        "# Use the total set of patches (or a representative subset) to compute the intersection of dates.\n",
        "common_dates = None\n",
        "for idx in patch_indices_total:\n",
        "    patch_path = os.path.join(path_out, f\"eopatch_{idx}\")\n",
        "    patch_dates = extract_dates_from_patch(patch_path)\n",
        "    if common_dates is None:\n",
        "        common_dates = patch_dates\n",
        "    else:\n",
        "        common_dates = common_dates.intersection(patch_dates)\n",
        "\n",
        "# Convert the common dates set to a sorted list\n",
        "date_list_sampled = sorted(common_dates)\n",
        "print(\"Common dates across patches:\", date_list_sampled)\n",
        "\n",
        "# --- STEP 2: Define Helper Functions for Processing and Aggregation ---\n",
        "\n",
        "def process_patch(patch_index, date_list, whc_class=2, divisor=100):\n",
        "    \"\"\"\n",
        "    Load a patch, apply masks, filter the timestamps by the given date list, and compute\n",
        "    per-date area (i.e. count of pixels with value==whc_class divided by divisor).\n",
        "\n",
        "    Returns:\n",
        "      filtered_times: numpy array of datetime objects (only for dates in date_list)\n",
        "      areas: numpy array of computed area values for each date\n",
        "    \"\"\"\n",
        "    patch_path = os.path.join(path_out, f\"eopatch_{patch_index}\")\n",
        "    eopatch = EOPatch.load(patch_path, lazy_loading=True)\n",
        "\n",
        "    # Apply the masks to the WHC data\n",
        "    mask_valid = eopatch.mask['IS_VALID'].astype(bool)\n",
        "    mask_water = eopatch.mask_timeless['NOMINAL_WATER'].astype(bool)\n",
        "    whc = eopatch.data['WHC'].copy()  # shape: (time, w, h, channels)\n",
        "\n",
        "    # Ensure mask_water has the same shape as whc\n",
        "    mask_water = np.broadcast_to(mask_water, whc.shape)\n",
        "\n",
        "    # Set invalid or non-water pixels to NaN\n",
        "    whc[~mask_valid] = np.nan\n",
        "    whc[~mask_water] = np.nan\n",
        "\n",
        "    # Get timestamps from the patch\n",
        "    times = np.array(eopatch.timestamps)\n",
        "\n",
        "    # Filter indices by checking if the date of each timestamp is in date_list\n",
        "    indices = [i for i, t in enumerate(times) if t.date() in date_list]\n",
        "\n",
        "    if not indices:\n",
        "        # If no timestamps match, return None values\n",
        "        return None, None\n",
        "\n",
        "    filtered_times = times[indices]\n",
        "\n",
        "    # Reshape WHC data: assume shape is (t, w, h, channels)\n",
        "    t_total, w, h, _ = whc.shape\n",
        "    whc_reshaped = whc.reshape(t_total, w * h)\n",
        "\n",
        "    # Only keep the filtered timestamps\n",
        "    whc_filtered = whc_reshaped[indices, :]\n",
        "\n",
        "    # Calculate area: count pixels equal to whc_class and divide by divisor\n",
        "    areas = (np.count_nonzero(whc_filtered == whc_class, axis=1)) / divisor\n",
        "\n",
        "    return filtered_times, areas\n",
        "\n",
        "def aggregate_over_patches(patch_indices, date_list, whc_class=2, divisor=100):\n",
        "    \"\"\"\n",
        "    Process multiple patches and sum their computed areas on a per-date basis.\n",
        "\n",
        "    Returns:\n",
        "      times: a numpy array of datetime objects (assumed identical for each patch)\n",
        "      aggregated: a numpy array of aggregated area values per date.\n",
        "    \"\"\"\n",
        "    results = []   # list to store the area arrays for each patch\n",
        "    common_times = None\n",
        "    for idx in patch_indices:\n",
        "        times, areas = process_patch(idx, date_list, whc_class, divisor)\n",
        "        if areas is not None:\n",
        "            results.append(areas)\n",
        "            if common_times is None:\n",
        "                common_times = times\n",
        "            # Check if the number of dates matches, otherwise fill with NaNs\n",
        "            elif len(areas) != len(common_times):\n",
        "                # Create an array of NaNs with the correct length\n",
        "                nan_array = np.full(len(common_times), np.nan)\n",
        "                # Replace NaNs with existing area values\n",
        "                nan_array[:len(areas)] = areas\n",
        "                results[-1] = nan_array  # Update results with filled array\n",
        "\n",
        "    if not results:\n",
        "        return None, None\n",
        "    # Stack results (each row corresponds to one patch) and sum along patches\n",
        "    results_array = np.vstack(results)\n",
        "    aggregated = np.sum(results_array, axis=0)\n",
        "    return common_times, aggregated\n",
        "\n",
        "# --- STEP 3: Aggregate the Areas for Each Group ---\n",
        "\n",
        "# Total area across all patches\n",
        "times_total, area_total = aggregate_over_patches(patch_indices_total, date_list_sampled,\n",
        "                                                 whc_class=whc_class_value, divisor=divisor)\n",
        "\n",
        "# Area for each designated group\n",
        "times_downstream, area_downstream = aggregate_over_patches(patch_indices_downstream, date_list_sampled,\n",
        "                                                           whc_class=whc_class_value, divisor=divisor)\n",
        "times_mid_upstream, area_mid_upstream = aggregate_over_patches(patch_indices_mid_upstream, date_list_sampled,\n",
        "                                                           whc_class=whc_class_value, divisor=divisor)\n",
        "times_upstream, area_upstream = aggregate_over_patches(patch_indices_upstream, date_list_sampled,\n",
        "                                                       whc_class=whc_class_value, divisor=divisor)\n",
        "times_midstream, area_midstream = aggregate_over_patches(patch_indices_midstream, date_list_sampled,\n",
        "                                                         whc_class=whc_class_value, divisor=divisor)\n",
        "\n",
        "# --- STEP 4: Create and Save Separate CSV Files ---\n",
        "\n",
        "def save_csv(times, areas, output_path):\n",
        "    \"\"\"Create a DataFrame with 'date' and 'area' columns and save it to CSV.\"\"\"\n",
        "    if times is None or areas is None:\n",
        "        print(f\"No data to save for {output_path}\")\n",
        "        return\n",
        "    df = pd.DataFrame({\"date\": times, \"Classified area WH (ha)\": areas})\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])  # ensure proper datetime format\n",
        "    df.set_index(\"date\", inplace=True)\n",
        "    df.to_csv(output_path)\n",
        "    print(f\"CSV saved to: {output_path}\")\n",
        "\n",
        "# # Define the new directory\n",
        "output_dir = \"./drive/My Drive/SentinelModel/datalists/\"\n",
        "\n",
        "# # Create the directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "# Save CSV for total area\n",
        "save_csv(times_total, area_total, \"./drive/My Drive/SentinelModel/datalists/total_area.csv\")\n",
        "\n",
        "# Save CSV for downstream area\n",
        "save_csv(times_mid_upstream, area_mid_upstream, \"./drive/My Drive/SentinelModel/datalists/mid_upstream_area.csv\")\n",
        "\n",
        "# Save CSV for downstream area\n",
        "save_csv(times_downstream, area_downstream, \"./drive/My Drive/SentinelModel/datalists/downstream_area.csv\")\n",
        "\n",
        "# Save CSV for upstream area\n",
        "save_csv(times_upstream, area_upstream, \"./drive/My Drive/SentinelModel/datalists/upstream_area.csv\")\n",
        "\n",
        "# Save CSV for midstream area\n",
        "save_csv(times_midstream, area_midstream, \"./drive/My Drive/SentinelModel/datalists/midstream_area.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04d3170",
      "metadata": {
        "id": "-vrJSL-e3idW"
      },
      "source": [
        "### 8.2 Visualize calculated area (ha) per section per data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bac62b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CBKW6dKetGMW",
        "outputId": "e17bec21-c05b-414f-8200-b03bb6511da0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- STEP 3: Format the Data Without Monthly Aggregation ---\n",
        "\n",
        "def format_raw_data(times, areas):\n",
        "    \"\"\"Format raw data into a DataFrame without resampling.\"\"\"\n",
        "    df = pd.DataFrame({\"date\": times, \"Classified area WH (ha)\": areas})\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"])  # Ensure proper datetime format\n",
        "    df.sort_values(\"date\", inplace=True)\n",
        "    return df\n",
        "\n",
        "# Total area across all patches\n",
        "df_total = format_raw_data(times_total, area_total)\n",
        "df_downstream = format_raw_data(times_downstream, area_downstream)\n",
        "df_upstream = format_raw_data(times_upstream, area_upstream)\n",
        "df_mid_upstream = format_raw_data(times_mid_upstream, area_mid_upstream)\n",
        "df_midstream = format_raw_data(times_midstream, area_midstream)\n",
        "\n",
        "# --- STEP 4: Create and Save Separate CSV Files ---\n",
        "\n",
        "def save_csv(df, output_path):\n",
        "    \"\"\"Save DataFrame to CSV.\"\"\"\n",
        "    if df is None or df.empty:\n",
        "        print(f\"No data to save for {output_path}\")\n",
        "        return\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"CSV saved to: {output_path}\")\n",
        "\n",
        "# Define output directory\n",
        "output_dir = \"./drive/My Drive/SentinelModel/datalists3/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Save CSVs\n",
        "save_csv(df_total, os.path.join(output_dir, \"total_area_raw.csv\"))\n",
        "save_csv(df_downstream, os.path.join(output_dir, \"downstream_area_raw.csv\"))\n",
        "save_csv(df_upstream, os.path.join(output_dir, \"upstream_area_raw.csv\"))\n",
        "save_csv(df_midstream, os.path.join(output_dir, \"midstream_area_raw.csv\"))\n",
        "save_csv(df_mid_upstream, os.path.join(output_dir, \"mid_upstream_area_raw.csv\"))\n",
        "\n",
        "# --- STEP 5: Plotting Per-Date Trends (No Monthly Aggregation) ---\n",
        "\n",
        "def plot_raw_trend(df, title):\n",
        "    \"\"\"Plot WH coverage as raw points by date.\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(df[\"date\"], df[\"Classified area WH (ha)\"], marker='o', linestyle='-', label=title)\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Classified area WH (ha)')\n",
        "    plt.title(title + \" (Raw Data)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot each section\n",
        "plot_raw_trend(df_total, 'Total Area WH Coverage')\n",
        "plot_raw_trend(df_downstream, 'Downstream WH Coverage')\n",
        "plot_raw_trend(df_upstream, 'Upstream WH Coverage')\n",
        "plot_raw_trend(df_midstream, 'Midstream WH Coverage')\n",
        "plot_raw_trend(df_mid_upstream, 'Middle upstream WH Coverage')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b36cd0f",
      "metadata": {
        "id": "ClX4KUBmiRBw"
      },
      "source": [
        "### 8.3 Plot mean WH coverage (ha) per section over a year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90944c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "MdOlU3tn9Eig",
        "outputId": "b9e39536-baf2-4d6e-82db-a3e1a1932f0a"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# 5 distance points = 4 segments\n",
        "boundary_distances = [110.0, 91.7, 73.2, 56.5, 37.5]  # example boundary points (km)\n",
        "segment_labels = [\"Upstream\", \"Upper midstream\", \"Lower midstream\", \"Downstream\"]\n",
        "\n",
        "# Midpoints of segments = where you plot WH values\n",
        "region_midpoints_km = [\n",
        "    (boundary_distances[i] + boundary_distances[i + 1]) / 2\n",
        "    for i in range(len(boundary_distances) - 1)\n",
        "]\n",
        "\n",
        "# Combine data into DataFrame\n",
        "df_all = pd.DataFrame({\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "})\n",
        "\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "# --- Plot ---\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot individual dates\n",
        "for i, date in enumerate(df_all.index):\n",
        "    values = df_all.loc[date].values\n",
        "    ax.plot(region_midpoints_km, values, color='gray', alpha=0.3, label=\"Scenes between 06/2024 and 06/2025\" if i == 0 else \"\")\n",
        "\n",
        "# Plot mean\n",
        "mean_values = df_all.mean().values\n",
        "ax.plot(region_midpoints_km, mean_values, marker='o', color='black', linewidth=2.5, label='Mean WH Coverage')\n",
        "\n",
        "# Title and axis labels\n",
        "start_date = df_all.index.min().strftime(\"%b %Y\")\n",
        "end_date = df_all.index.max().strftime(\"%b %Y\")\n",
        "ax.set_xlabel(\"Distance to Gulf of Thailand (km)\")\n",
        "ax.set_ylabel(\"Classified WH Area (ha)\")\n",
        "ax.grid(True)\n",
        "ax.legend(loc=\"upper right\")\n",
        "\n",
        "# X-ticks: boundary distances\n",
        "ax.set_xticks(boundary_distances)\n",
        "ax.set_xticklabels([f\"{d:.0f} km\" for d in boundary_distances])\n",
        "ax.invert_xaxis()  # Upstream on the left\n",
        "\n",
        "# Add secondary axis for segment names (between ticks)\n",
        "ax2 = ax.secondary_xaxis('top')\n",
        "ax2.set_xticks(region_midpoints_km)\n",
        "ax2.set_xticklabels(segment_labels)\n",
        "ax2.set_xlabel(\"River Segments\")\n",
        "\n",
        "# Save and show\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/Yearly_wh_absolute_coverage.png\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(output_path, dpi=300)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a2b80e",
      "metadata": {
        "id": "YfXf-ujIi4rK"
      },
      "source": [
        "### 8.4 Plot WH coverage (ha) over time per section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f5629be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "FqIdgfUN7zCv",
        "outputId": "be0f5a76-3268-454e-bf47-865538075001"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "# --- Build df_all from raw WH classified area (ha) ---\n",
        "df_all = pd.DataFrame({\n",
        "    \"Total area\": df_total.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"],\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"]\n",
        "}).sort_index()\n",
        "\n",
        "# Create color map\n",
        "colors = cm.get_cmap('tab10')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, col in enumerate(df_all.columns):\n",
        "    plt.plot(df_all.index, df_all[col],\n",
        "             marker='o', linestyle='-',\n",
        "             label=col, color=colors(i))\n",
        "\n",
        "# Highlight study period: 17 March to 26 April\n",
        "start_study = pd.to_datetime(\"2025-03-17\")\n",
        "end_study = pd.to_datetime(\"2025-04-26\")\n",
        "plt.axvspan(start_study, end_study, color='orange', alpha=0.2, label=\"Study Period\")\n",
        "\n",
        "# Styling\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Classified WH Area (ha)\")\n",
        "plt.title(\"Temporal Trends of WH Coverage Across River Segments\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figure\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/yearly_absolute_wh_coverage_vs_time.png\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "print(f\"Figure saved to: {output_path}\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b145a45c",
      "metadata": {
        "id": "Eiyf_gFojFMo"
      },
      "source": [
        "## 9 Plot WH coverage results as percentage of total area\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfbb61e",
      "metadata": {
        "id": "MThnYXgjjQLO"
      },
      "source": [
        "### 9.1 Plot WH percentage over distance, during a year per section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed91829",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "LAdx_gcWAfJJ",
        "outputId": "c660d6d8-ba61-4b36-87e1-056e458d07d2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.lines import Line2D\n",
        "import os\n",
        "\n",
        "# --- Define boundary distances and segment names ---\n",
        "boundary_distances = [110.0, 91.7, 73.2, 56.5, 37.5]  # 5 distances = 4 segments\n",
        "segment_labels = [\"Upstream\", \"Upper Midstream\", \"Lower Midstream\", \"Downstream\"]\n",
        "\n",
        "# --- Segment midpoints for plotting WH coverage ---\n",
        "region_midpoints_km = [\n",
        "    (boundary_distances[i] + boundary_distances[i + 1]) / 2\n",
        "    for i in range(len(boundary_distances) - 1)\n",
        "]\n",
        "\n",
        "# --- Segment areas (ha) for percentage conversion ---\n",
        "segment_areas = {\n",
        "    \"Upstream\": 678,\n",
        "    \"Mid-Upstream\": 405,\n",
        "    \"Midstream\": 451,\n",
        "    \"Downstream\": 472\n",
        "}\n",
        "\n",
        "# --- Calculate percentage WH coverage ---\n",
        "df_all = pd.DataFrame({\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Upstream\"] * 100,\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Mid-Upstream\"] * 100,\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Midstream\"] * 100,\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Downstream\"] * 100,\n",
        "})\n",
        "\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "# --- Plot setup ---\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot each individual date (gray lines)\n",
        "for i, date in enumerate(df_all.index):\n",
        "    values = df_all.loc[date].values\n",
        "    ax.plot(region_midpoints_km, values, color='gray', alpha=0.3)\n",
        "\n",
        "# Plot mean WH % coverage (bold black line with dots)\n",
        "mean_values = df_all.mean().values\n",
        "ax.plot(region_midpoints_km, mean_values, marker='o', color='black', linewidth=2.5)\n",
        "\n",
        "# --- Legend ---\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], color='gray', alpha=0.3, linewidth=2, label='Scenes between 06/2024 and 06/2025'),\n",
        "    Line2D([0], [0], color='black', marker='o', linewidth=2.5, label='Mean WH % Coverage')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc=\"upper right\")\n",
        "\n",
        "# --- Axis Labels & Grid ---\n",
        "start = df_all.index.min().strftime('%b %Y')\n",
        "end = df_all.index.max().strftime('%b %Y')\n",
        "ax.set_xlabel(\"Distance to Gulf of Thailand (km)\")\n",
        "ax.set_ylabel(\"WH Coverage (% of river segment)\")\n",
        "ax.set_xticks(boundary_distances)\n",
        "ax.set_xticklabels([f\"{d:.0f} km\" for d in boundary_distances])\n",
        "ax.invert_xaxis()\n",
        "ax.grid(True)\n",
        "\n",
        "# --- Add segment names above axis ---\n",
        "ax2 = ax.secondary_xaxis('top')\n",
        "ax2.set_xticks(region_midpoints_km)\n",
        "ax2.set_xticklabels(segment_labels)\n",
        "ax2.set_xlabel(\"River Segments\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save ---\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/yearly_WH_percentage_coverage.jpg\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9980dd68",
      "metadata": {
        "id": "vSQfiwV0jd9f"
      },
      "source": [
        "### 9.2 Plot WH percentage over distance, during the study period per section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece26d75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "YQY9Hat1sONC",
        "outputId": "50203416-e010-4794-c0dc-386b3d0c2300"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.lines import Line2D\n",
        "import os\n",
        "\n",
        "# --- Define boundary distances and segment names ---\n",
        "boundary_distances = [110.0, 91.7, 73.2, 56.5, 37.5]  # 5 distances = 4 segments\n",
        "segment_labels = [\"Upstream\", \"Upper Midstream\", \"Lower Midstream\", \"Downstream\"]\n",
        "\n",
        "# --- Segment midpoints for plotting WH coverage ---\n",
        "region_midpoints_km = [\n",
        "    (boundary_distances[i] + boundary_distances[i + 1]) / 2\n",
        "    for i in range(len(boundary_distances) - 1)\n",
        "]\n",
        "\n",
        "# --- Segment areas (ha) for percentage conversion ---\n",
        "segment_areas = {\n",
        "    \"Upstream\": 678,\n",
        "    \"Mid-Upstream\": 405,\n",
        "    \"Midstream\": 451,\n",
        "    \"Downstream\": 472\n",
        "}\n",
        "\n",
        "# --- Calculate percentage WH coverage ---\n",
        "df_all = pd.DataFrame({\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Upstream\"] * 100,\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Mid-Upstream\"] * 100,\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Midstream\"] * 100,\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Downstream\"] * 100,\n",
        "})\n",
        "\n",
        "# --- Filter for March and April 2025 ---\n",
        "start = pd.to_datetime(\"2025-03-01\")\n",
        "end = pd.to_datetime(\"2025-05-01\")\n",
        "df_all = df_all[(df_all.index >= start) & (df_all.index <= end)]\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "# --- Plot setup ---\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Plot each individual date (gray lines)\n",
        "for i, date in enumerate(df_all.index):\n",
        "    values = df_all.loc[date].values\n",
        "    ax.plot(region_midpoints_km, values, color='gray', alpha=0.3)\n",
        "\n",
        "# Plot mean WH % coverage (bold black line with dots)\n",
        "mean_values = df_all.mean().values\n",
        "ax.plot(region_midpoints_km, mean_values, marker='o', color='black', linewidth=2.5)\n",
        "\n",
        "# --- Legend ---\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], color='gray', alpha=0.3, linewidth=2, label='Scenes from Marâ€“Apr 2025'),\n",
        "    Line2D([0], [0], color='black', marker='o', linewidth=2.5, label='Mean WH % Coverage')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc=\"upper right\")\n",
        "\n",
        "# --- Axis Labels & Grid ---\n",
        "ax.set_xlabel(\"Distance to Gulf of Thailand (km)\")\n",
        "ax.set_ylabel(\"WH Coverage (% of river segment)\")\n",
        "ax.set_xticks(boundary_distances)\n",
        "ax.set_xticklabels([f\"{d:.0f} km\" for d in boundary_distances])\n",
        "ax.invert_xaxis()\n",
        "ax.grid(True)\n",
        "\n",
        "# --- Add segment names above axis ---\n",
        "ax2 = ax.secondary_xaxis('top')\n",
        "ax2.set_xticks(region_midpoints_km)\n",
        "ax2.set_xticklabels(segment_labels)\n",
        "ax2.set_xlabel(\"River Segments\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save ---\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/WH_coverage_Mar_Apr_2025.jpg\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5bc520",
      "metadata": {
        "id": "x8MVWQADj8IZ"
      },
      "source": [
        "### 9.3 Plot WH seasonality in percentage over a year per section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28c7f7ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "id": "lQdrFSvaNCbi",
        "outputId": "13dffd70-f81f-497a-b656-96608e14efee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import os\n",
        "\n",
        "# Total segment areas in hectares\n",
        "segment_areas = {\n",
        "    \"Total area\": 2006,\n",
        "    \"Downstream\": 472,\n",
        "    \"Mid-Upstream\": 405,\n",
        "    \"Midstream\": 451,\n",
        "    \"Upstream\": 678\n",
        "}\n",
        "\n",
        "# --- Correct percentage WH coverage per segment ---\n",
        "df_pct = pd.DataFrame({\n",
        "    \"total area\": df_total.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Total area\"] * 100,\n",
        "    # \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Upstream\"] * 100,\n",
        "    # \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Mid-Upstream\"] * 100,\n",
        "    # \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Midstream\"] * 100,\n",
        "    # \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Downstream\"] * 100,\n",
        "}).sort_index()\n",
        "\n",
        "# --- Plot ---\n",
        "colors = cm.get_cmap('tab10')\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "for i, col in enumerate(df_pct.columns):\n",
        "    ax.plot(df_pct.index, df_pct[col],\n",
        "             marker='o', linestyle='-',\n",
        "             label=col, color=colors(i))\n",
        "\n",
        "# Highlight study period\n",
        "start_study = pd.to_datetime(\"2025-03-17\")\n",
        "end_study = pd.to_datetime(\"2025-04-26\")\n",
        "ax.axvspan(start_study, end_study, color='orange', alpha=0.2, label=\"Study Period\")\n",
        "\n",
        "# --- Styling ---\n",
        "ax.set_xlabel(\"Date\", fontsize=14)\n",
        "ax.set_ylabel(\"WH Coverage [%]\", fontsize=14)\n",
        "ax.tick_params(axis='both', labelsize=12)\n",
        "ax.grid(True, linestyle='--', alpha=0.5)\n",
        "ax.legend(title=\"River Segment\", fontsize=10, title_fontsize=11)\n",
        "fig.autofmt_xdate(rotation=45)\n",
        "ax.legend(title=\"River Segment\", fontsize=10, title_fontsize=11, loc='upper left', bbox_to_anchor=(0.25, 1))\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save figure ---\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/yearly_wh_coverage_percentage_vs_time_no_title.png\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "print(f\"Figure saved to: {output_path}\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea13f655",
      "metadata": {
        "id": "lRyN25O7kOGx"
      },
      "source": [
        "### 9.4 Retrieve summary of WH percentages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6017c00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KedK46U89e6",
        "outputId": "77fa11cd-8407-4265-f0f0-79caf4019335"
      },
      "outputs": [],
      "source": [
        "# --- Compute total river WH coverage (% weighted by area) ---\n",
        "weights = {\n",
        "    seg: segment_areas[seg] / segment_areas[\"Total area\"]\n",
        "    for seg in [\"Upstream\", \"Mid-Upstream\", \"Midstream\", \"Downstream\"]\n",
        "}\n",
        "df_pct[\"Total River\"] = sum(df_pct[seg] * weight for seg, weight in weights.items())\n",
        "\n",
        "# --- Compute statistics ---\n",
        "summary_stats = df_pct[[\"Upstream\", \"Mid-Upstream\", \"Midstream\", \"Downstream\", \"Total River\"]].agg([\"mean\", \"min\", \"max\"])\n",
        "\n",
        "# --- Display results ---\n",
        "print(\"\\n--- WH Coverage Summary Statistics [%] ---\")\n",
        "for stat in [\"mean\", \"min\", \"max\"]:\n",
        "    print(f\"\\n{stat.capitalize()} WH coverage:\")\n",
        "    for segment in summary_stats.columns:\n",
        "        value = summary_stats.loc[stat, segment]\n",
        "        print(f\"{segment}: {value:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64c16c8",
      "metadata": {
        "id": "Dgwmr1lnkfbC"
      },
      "source": [
        "### 9.5 Plot WH seasonality in percentage over study period per section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9451eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "v3SkzkZvBu0B",
        "outputId": "e0e2d0a0-1220-49fb-fe6b-72e3685b7b5b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import os\n",
        "\n",
        "# Total segment areas in hectares\n",
        "segment_areas = {\n",
        "    \"Downstream\": 472,\n",
        "    \"Mid-Upstream\": 405,\n",
        "    \"Midstream\": 451,\n",
        "    \"Upstream\": 678\n",
        "}\n",
        "\n",
        "# --- Compute WH coverage as percentage ---\n",
        "df_pct = pd.DataFrame({\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Upstream\"] * 100,\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Mid-Upstream\"] * 100,\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Midstream\"] * 100,\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Downstream\"] * 100,\n",
        "}).sort_index()\n",
        "\n",
        "# --- Filter for March and April 2025 ---\n",
        "start = pd.to_datetime(\"2025-03-01\")\n",
        "end = pd.to_datetime(\"2025-04-30\")\n",
        "df_pct = df_pct[(df_pct.index >= start) & (df_pct.index <= end)]\n",
        "\n",
        "# --- Plot ---\n",
        "colors = cm.get_cmap('tab10')\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, col in enumerate(df_pct.columns):\n",
        "    plt.plot(df_pct.index, df_pct[col],\n",
        "             marker='o', linestyle='-',\n",
        "             label=col, color=colors(i))\n",
        "\n",
        "# Highlight study period: 17 March to 26 April\n",
        "start_study = pd.to_datetime(\"2025-03-17\")\n",
        "end_study = pd.to_datetime(\"2025-04-26\")\n",
        "plt.axvspan(start_study, end_study, color='orange', alpha=0.2, label=\"Study Period\")\n",
        "\n",
        "# --- Styling ---\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"WH Coverage (% of river segment)\")\n",
        "plt.title(\"Temporal Trends of WH Coverage (Marâ€“Apr 2025)\")\n",
        "plt.grid(True)\n",
        "plt.legend(title=\"River Segment\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Save figure ---\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/wh_coverage_mar_apr_2025.png\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "print(f\"Figure saved to: {output_path}\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c16ec9",
      "metadata": {
        "id": "X4__9ieKku8G"
      },
      "source": [
        "## 10. Create main figures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5babae",
      "metadata": {
        "id": "wrtwh0V1k1gE"
      },
      "source": [
        "### 10.1 Combine sentinel-2 with object-detection results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ce26aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "7ZgX0P-JuxDN",
        "outputId": "446e293a-0faf-4940-e20f-edfa45a84638"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import os\n",
        "\n",
        "# --- Assume df_upstream, df_mid_upstream, df_midstream, df_downstream are already loaded with 'date' and 'Classified area WH (ha)' columns ---\n",
        "\n",
        "boundary_distances = [110.0, 91.7, 73.2, 56.5, 37.5]\n",
        "region_midpoints_km = [(boundary_distances[i] + boundary_distances[i + 1]) / 2 for i in range(len(boundary_distances) - 1)]\n",
        "segment_labels = [\"Upstream\", \"Upper Midstream\", \"Lower Midstream\", \"Downstream\"]\n",
        "segment_areas = {\"Upstream\": 678, \"Mid-Upstream\": 405, \"Midstream\": 451, \"Downstream\": 472}\n",
        "\n",
        "# --- Calculate percentage WH coverage ---\n",
        "df_all_timeseries = pd.DataFrame({\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Upstream\"] * 100,\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Mid-Upstream\"] * 100,\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Midstream\"] * 100,\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Downstream\"] * 100,\n",
        "})\n",
        "\n",
        "# Filter Marchâ€“April 2025\n",
        "start = pd.to_datetime(\"2025-03-01\")\n",
        "end = pd.to_datetime(\"2025-05-01\")\n",
        "df_all_timeseries = df_all_timeseries[(df_all_timeseries.index >= start) & (df_all_timeseries.index <= end)]\n",
        "df_all_timeseries.dropna(inplace=True)\n",
        "\n",
        "# Compute mean, min, max\n",
        "mean_values = df_all_timeseries.mean().values\n",
        "min_values = df_all_timeseries.min().values\n",
        "max_values = df_all_timeseries.max().values\n",
        "\n",
        "# Extend to boundary points\n",
        "extended_x = [boundary_distances[0]] + region_midpoints_km + [boundary_distances[-1]]\n",
        "extended_y = [mean_values[0]] + list(mean_values) + [mean_values[-1]]\n",
        "extended_min_y = [min_values[0]] + list(min_values) + [min_values[-1]]\n",
        "extended_max_y = [max_values[0]] + list(max_values) + [max_values[-1]]\n",
        "\n",
        "# --- UAV/Bridge data ---\n",
        "uav_data = {\n",
        "    'ID': ['UAV1', 'UAV2', 'UAV3', 'UAV4'],\n",
        "    'WH_mean': [2.971221, 4.338245, 10.67053, 3.35331],\n",
        "    'WH_min': [1.502088, 2.562471, 7.256699, 2.053657],\n",
        "    'WH_max': [4.440355, 6.114019, 14.08436, 4.652962]\n",
        "}\n",
        "uav_df = pd.DataFrame(uav_data)\n",
        "uav_df['Type'] = 'UAV imagery'\n",
        "\n",
        "bridge_data = {\n",
        "    'ID': ['B1', 'B2', 'B3', 'B4'],\n",
        "    'WH_mean': [8.28907, 0.260304, 0.376089, 0.098669],\n",
        "    'WH_min': [3.849058, 0.175941, 0.06173, 0.087226],\n",
        "    'WH_max': [10.90691, 0.445814, 0.844407, 0.124957]\n",
        "}\n",
        "bridge_df = pd.DataFrame(bridge_data)\n",
        "bridge_df['Type'] = 'Bridge imagery'\n",
        "\n",
        "distances = pd.DataFrame({\n",
        "    'ID': ['UAV1', 'UAV2', 'UAV3', 'UAV4', 'B1', 'B2', 'B3', 'B4'],\n",
        "    'Distance': [104.5, 97.8, 95.3, 92.2, 72.4, 64.6, 54.3, 42.4]\n",
        "})\n",
        "\n",
        "df_all_uav_bridge = pd.concat([uav_df, bridge_df], ignore_index=True)\n",
        "df_all_uav_bridge = df_all_uav_bridge.merge(distances, on='ID')\n",
        "df_all_uav_bridge = df_all_uav_bridge.sort_values(by='Distance', ascending=False)\n",
        "\n",
        "# --- Plot ---\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "# Range band and mean line\n",
        "ax.fill_between(extended_x, extended_min_y, extended_max_y, color='gray', alpha=0.3)\n",
        "ax.plot(extended_x, extended_y, color='black', linewidth=2.5)\n",
        "ax.plot(region_midpoints_km, mean_values, 'o', color='black')\n",
        "\n",
        "# UAV/Bridge points\n",
        "ax.plot(df_all_uav_bridge['Distance'], df_all_uav_bridge['WH_mean'], linestyle='-', color='#4CAF50', linewidth=1.5)\n",
        "for i, row in df_all_uav_bridge.iterrows():\n",
        "    ax.errorbar(row['Distance'], row['WH_mean'],\n",
        "                yerr=[[row['WH_mean'] - row['WH_min']], [row['WH_max'] - row['WH_mean']]],\n",
        "                fmt='o' if row['Type'] == 'UAV imagery' else 's',\n",
        "                markersize=6, color='#4CAF50', ecolor='#4CAF50', elinewidth=1)\n",
        "    ax.text(row['Distance'], row['WH_mean'] * 1.2, row['ID'], fontsize=8)\n",
        "\n",
        "# Axes formatting\n",
        "ax.set_xlabel('Distance to Gulf of Thailand [km]', fontsize=10)\n",
        "ax.set_ylabel('WH Coverage [%]', fontsize=10)\n",
        "ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "ax.set_yscale('log')\n",
        "ax.invert_xaxis()\n",
        "ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Boundary lines\n",
        "for bd in boundary_distances:\n",
        "    ax.axvline(x=bd, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Segment labels\n",
        "ax2 = ax.secondary_xaxis('top')\n",
        "ax2.set_xticks(region_midpoints_km)\n",
        "ax2.set_xticklabels(segment_labels, fontsize=8)\n",
        "ax2.set_xlabel('River Segments | Marâ€“Apr 2025', fontsize=10)\n",
        "\n",
        "# Legend\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], color='gray', alpha=0.3, linewidth=6, label='Sentinel-2: min-max'),\n",
        "    Line2D([0], [0], color='black', marker='o', linewidth=2, label='Sentinel-2: mean (n=5)'),\n",
        "    Line2D([0], [0], marker='o', color='w', markerfacecolor='#4CAF50', markersize=8, label='UAV imagery'),\n",
        "    Line2D([0], [0], marker='s', color='w', markerfacecolor='#4CAF50', markersize=8, label='Bridge imagery')\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figure\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/wh_coverage_mar_apr_2025_all_methods.png\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "plt.show()\n",
        "print(f\"Figure saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1592d5",
      "metadata": {
        "id": "Yl0ei_NBk9Qr"
      },
      "source": [
        "###10.2 Plot WH percentage over distance, during a year per section, revised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce4c4c75",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "0KdP7W3A2nOJ",
        "outputId": "61594841-2350-4280-dcbf-647869fe1194"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.lines import Line2D\n",
        "import os\n",
        "\n",
        "# --- Define boundary distances and segment names ---\n",
        "boundary_distances = [110.0, 91.7, 73.2, 56.5, 37.5]\n",
        "segment_labels = [\"Upstream\", \"Upper Midstream\", \"Lower Midstream\", \"Downstream\"]\n",
        "region_midpoints_km = [(boundary_distances[i] + boundary_distances[i + 1]) / 2 for i in range(len(boundary_distances) - 1)]\n",
        "segment_areas = {\"Upstream\": 678, \"Mid-Upstream\": 405, \"Midstream\": 451, \"Downstream\": 472}\n",
        "\n",
        "# --- Calculate percentage WH coverage ---\n",
        "df_all = pd.DataFrame({\n",
        "    \"Upstream\": df_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Upstream\"] * 100,\n",
        "    \"Mid-Upstream\": df_mid_upstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Mid-Upstream\"] * 100,\n",
        "    \"Midstream\": df_midstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Midstream\"] * 100,\n",
        "    \"Downstream\": df_downstream.set_index(\"date\")[\"Classified area WH (ha)\"] / segment_areas[\"Downstream\"] * 100,\n",
        "})\n",
        "df_all.dropna(inplace=True)\n",
        "\n",
        "# Compute mean, min, max\n",
        "mean_values = df_all.mean().values\n",
        "min_values = df_all.min().values\n",
        "max_values = df_all.max().values\n",
        "\n",
        "# Extend to boundaries\n",
        "extended_x = [boundary_distances[0]] + region_midpoints_km + [boundary_distances[-1]]\n",
        "extended_mean_y = [mean_values[0]] + list(mean_values) + [mean_values[-1]]\n",
        "extended_min_y = [min_values[0]] + list(min_values) + [min_values[-1]]\n",
        "extended_max_y = [max_values[0]] + list(max_values) + [max_values[-1]]\n",
        "\n",
        "# --- Plot ---\n",
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "\n",
        "# Fill range band\n",
        "ax.fill_between(extended_x, extended_min_y, extended_max_y, color='gray', alpha=0.3, label='Range (Min-Max)')\n",
        "\n",
        "# Plot mean WH % coverage (bold black line with dots)\n",
        "ax.plot(extended_x, extended_mean_y, color='black', linewidth=2.5)\n",
        "ax.plot(region_midpoints_km, mean_values, 'o', color='black')\n",
        "\n",
        "# Axes formatting\n",
        "ax.set_xlabel('Distance to Gulf of Thailand [km]', fontsize=10)\n",
        "ax.set_ylabel('WH Coverage [%]', fontsize=10)\n",
        "ax.tick_params(axis='both', which='major', labelsize=8)\n",
        "ax.invert_xaxis()\n",
        "ax.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Boundary lines\n",
        "for bd in boundary_distances:\n",
        "    ax.axvline(x=bd, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Segment labels\n",
        "ax2 = ax.secondary_xaxis('top')\n",
        "ax2.set_xticks(region_midpoints_km)\n",
        "ax2.set_xticklabels(segment_labels, fontsize=8)\n",
        "ax2.set_xlabel('River Segments | June 2024 -June 2025', fontsize=10)\n",
        "\n",
        "# Legend\n",
        "legend_elements = [\n",
        "    Line2D([0], [0], color='gray', alpha=0.3, linewidth=6, label='Sentinel-2: min-max'),\n",
        "    Line2D([0], [0], color='black', marker='o', linewidth=2, label='Sentinel-2: mean (n=27)'),\n",
        "]\n",
        "ax.legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figure\n",
        "output_path = \"./drive/My Drive/SentinelModel/figures/yearly_WH_percentage_coverage.png\"\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "plt.savefig(output_path, dpi=300)\n",
        "plt.show()\n",
        "\n",
        "n = len(df_all.index)\n",
        "print(f\"Number of gray lines plotted: {n}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77d3ce19",
      "metadata": {
        "id": "MxtoeQF2lbn5"
      },
      "source": [
        "##11 Perform statistical analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bf53f26",
      "metadata": {
        "id": "drg9eT9_ll-t"
      },
      "source": [
        "###11.1 Distance vs WH coverage (over a year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c01b68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av__iyk4_ui7",
        "outputId": "2c529779-b968-4b15-9b00-a5aa8f19883d"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Define distances from Gulf of Thailand for each segment midpoint (same order as df_all columns)\n",
        "segment_distances = {\n",
        "    \"Upstream\": region_midpoints_km[0],\n",
        "    \"Mid-Upstream\": region_midpoints_km[1],\n",
        "    \"Midstream\": region_midpoints_km[2],\n",
        "    \"Downstream\": region_midpoints_km[3]\n",
        "}\n",
        "\n",
        "# Convert df_all (27 rows Ã— 4 segments) into long format for correlation\n",
        "df_long = df_all.reset_index().melt(id_vars=\"date\", var_name=\"segment\", value_name=\"wh_percent\")\n",
        "df_long[\"distance_km\"] = df_long[\"segment\"].map(segment_distances)\n",
        "\n",
        "# Compute Spearman correlation across all 27 Ã— 4 = 108 values\n",
        "rho, pval = spearmanr(df_long[\"wh_percent\"], df_long[\"distance_km\"])\n",
        "\n",
        "# Display results\n",
        "print(\"\\n--- Spearman Correlation ---\")\n",
        "print(f\"Ï = {rho:.2f}, p = {pval:.4f}, n = {len(df_long)}\")\n",
        "\n",
        "# --- Compute mean, min, and max WH coverage per segment ---\n",
        "coverage_stats = df_all.agg([\"mean\", \"min\", \"max\"]).round(2)\n",
        "\n",
        "# --- Compute area-weighted total WH coverage (mean, min, max) ---\n",
        "segment_areas_only = {\n",
        "    \"Upstream\": 678,\n",
        "    \"Mid-Upstream\": 405,\n",
        "    \"Midstream\": 451,\n",
        "    \"Downstream\": 472\n",
        "}\n",
        "total_area = sum(segment_areas_only.values())\n",
        "\n",
        "# Create weights for each segment\n",
        "weights = {seg: area / total_area for seg, area in segment_areas_only.items()}\n",
        "\n",
        "# Compute weighted stats\n",
        "weighted_mean = sum(df_all[seg].mean() * weights[seg] for seg in df_all.columns)\n",
        "weighted_min = sum(df_all[seg].min() * weights[seg] for seg in df_all.columns)\n",
        "weighted_max = sum(df_all[seg].max() * weights[seg] for seg in df_all.columns)\n",
        "\n",
        "# --- Print results ---\n",
        "print(\"\\n--- WH Coverage Statistics [%] ---\")\n",
        "for stat in [\"mean\", \"min\", \"max\"]:\n",
        "    print(f\"\\n{stat.capitalize()} WH coverage by segment:\")\n",
        "    for segment in df_all.columns:\n",
        "        value = coverage_stats.loc[stat, segment]\n",
        "        print(f\"  {segment}: {value:.2f}%\")\n",
        "\n",
        "print(f\"\\nTotal River (area-weighted):\")\n",
        "print(f\"  Mean: {weighted_mean:.2f}%\")\n",
        "print(f\"  Min:  {weighted_min:.2f}%\")\n",
        "print(f\"  Max:  {weighted_max:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21ab53f1",
      "metadata": {
        "id": "AQe43q3jl43u"
      },
      "source": [
        "### 11.2 Sentinel-2 vs Object detection (in study period)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3633ff3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUeU0Pvl4jEo",
        "outputId": "0d411047-bcbc-43cd-80a7-b218a4fba1a0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# --- 1. Sentinel-2 Segment Means ---\n",
        "# Assumes df_all_timeseries is already filtered for Marâ€“Apr 2025\n",
        "\n",
        "# Compute mean WH% per segment\n",
        "sentinel_means = df_all_timeseries.mean().round(2).to_dict()\n",
        "print(\"Sentinel WH% Means by Segment:\", sentinel_means)\n",
        "\n",
        "# Segment distances (midpoints)\n",
        "sentinel_segment_distances = {\n",
        "    \"Upstream\": 100.85,\n",
        "    \"Mid-Upstream\": 82.45,\n",
        "    \"Midstream\": 64.85,\n",
        "    \"Downstream\": 47.0\n",
        "}\n",
        "\n",
        "# Segment areas (hectares)\n",
        "segment_areas = {\n",
        "    \"Upstream\": 678,\n",
        "    \"Mid-Upstream\": 405,\n",
        "    \"Midstream\": 451,\n",
        "    \"Downstream\": 472\n",
        "}\n",
        "\n",
        "# Total area\n",
        "total_area = sum(segment_areas.values())\n",
        "\n",
        "# Build Sentinel dataframe\n",
        "sentinel_df = pd.DataFrame({\n",
        "    \"Segment\": sentinel_means.keys(),\n",
        "    \"WH_mean\": sentinel_means.values()\n",
        "})\n",
        "sentinel_df[\"Distance\"] = sentinel_df[\"Segment\"].map(sentinel_segment_distances)\n",
        "sentinel_df = sentinel_df.dropna().reset_index(drop=True).round(2)\n",
        "\n",
        "print(\"\\nâœ… Sentinel data:\")\n",
        "print(sentinel_df)\n",
        "\n",
        "# --- 2. UAV/Bridge Segment Assignment and Mean ---\n",
        "df_all_uav_bridge['Segment'] = df_all_uav_bridge['ID'].map({\n",
        "    'UAV1': 'Upstream', 'UAV2': 'Upstream', 'UAV3': 'Upstream', 'UAV4': 'Upstream',\n",
        "    'B1': 'Midstream', 'B2': 'Midstream',\n",
        "    'B3': 'Downstream', 'B4': 'Downstream'\n",
        "})\n",
        "\n",
        "# Group by segment\n",
        "uav_bridge_segment_means = (\n",
        "    df_all_uav_bridge\n",
        "    .groupby('Segment')\n",
        "    .agg({'WH_mean': 'mean', 'Distance': 'mean'})\n",
        "    .reset_index()\n",
        "    .round(2)\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… UAV/Bridge data (grouped):\")\n",
        "print(uav_bridge_segment_means)\n",
        "\n",
        "# --- 3. Spearman Correlations ---\n",
        "\n",
        "# Sentinel WH_mean vs Distance\n",
        "rho_sentinel, p_sentinel = spearmanr(sentinel_df['WH_mean'], sentinel_df['Distance'])\n",
        "\n",
        "# UAV/Bridge WH_mean vs Distance\n",
        "rho_uav, p_uav = spearmanr(uav_bridge_segment_means['WH_mean'], uav_bridge_segment_means['Distance'])\n",
        "\n",
        "# Merge for cross-comparison (ignore Mid-Upstream if missing)\n",
        "combined = pd.merge(\n",
        "    sentinel_df,\n",
        "    uav_bridge_segment_means,\n",
        "    on='Segment',\n",
        "    suffixes=('_sentinel', '_uav')\n",
        ").dropna()\n",
        "\n",
        "# Spearman: Sentinel vs UAV/Bridge WH_mean\n",
        "rho_cross, p_cross = spearmanr(combined['WH_mean_sentinel'], combined['WH_mean_uav'])\n",
        "\n",
        "# --- 4. Print Spearman Results ---\n",
        "print(\"\\nðŸ“Œ Spearman Correlation Results:\")\n",
        "print(f\"Sentinel WH_mean vs Distance:     Ï = {rho_sentinel:.2f}, p = {p_sentinel:.4f}\")\n",
        "print(f\"UAV/Bridge WH_mean vs Distance:   Ï = {rho_uav:.2f}, p = {p_uav:.4f}\")\n",
        "print(f\"Sentinel vs UAV/Bridge WH_mean:   Ï = {rho_cross:.2f}, p = {p_cross:.4f}\")\n",
        "\n",
        "# --- 5. UAV/Bridge Comparison to Sentinel Values ---\n",
        "print(\"\\nðŸ“Š Segment-Level UAV/Bridge Comparison:\")\n",
        "\n",
        "for segment in ['Upstream', 'Midstream', 'Downstream']:\n",
        "    if segment in sentinel_means:\n",
        "        ref = sentinel_means[segment]\n",
        "        subset = df_all_uav_bridge[df_all_uav_bridge['Segment'] == segment]\n",
        "        print(f\"\\nSentinel-2 {segment} Mean WH%: {ref:.2f}\")\n",
        "        for _, row in subset.iterrows():\n",
        "            factor = row['WH_mean'] / ref if ref else None\n",
        "            print(f\"{row['ID']} mean WH% is {factor:.1f}x {'higher' if factor >= 1 else 'lower'} than Sentinel-2\")\n",
        "\n",
        "# --- 6. Total WH Area and Area-Weighted WH% ---\n",
        "total_wh_area_ha = sum(\n",
        "    sentinel_means[segment] / 100 * segment_areas[segment]\n",
        "    for segment in sentinel_means\n",
        ")\n",
        "\n",
        "total_wh_pct = (total_wh_area_ha / total_area) * 100\n",
        "\n",
        "print(f\"\\nðŸ“¦ Overall average WH-covered area (ha): {total_wh_area_ha:.2f} ha\")\n",
        "print(f\"ðŸ“Š Overall average WH% across river (area-weighted): {total_wh_pct:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a730709",
      "metadata": {
        "id": "8jaZsZe5mbV4"
      },
      "source": [
        "### 11.3 Print values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d094ed4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktQ0-v3SegC5",
        "outputId": "2ab91ad8-4d6a-4cce-df55-5dbefc86e616"
      },
      "outputs": [],
      "source": [
        "print(\"Sentinel values:\")\n",
        "print(sentinel_df)\n",
        "\n",
        "print(\"\\nUAV/Bridge values:\")\n",
        "print(uav_bridge_segment_means)\n",
        "\n",
        "print(\"\\nCombined for cross-correlation:\")\n",
        "print(combined[['Segment', 'WH_mean_sentinel', 'WH_mean_uav']])\n",
        "print(df_all_uav_bridge[['ID', 'WH_mean', 'Distance']].sort_values(by='Distance', ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f083b41",
      "metadata": {
        "id": "36IfZIE0mZ1W"
      },
      "source": [
        "### 11.4 Object detection vs Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e9b403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q491LlD9f6hd",
        "outputId": "fb76aa40-fc20-4cbb-8077-197ce5c93e9e"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Raw UAV/Bridge Spearman correlation (n = 8)\n",
        "rho_raw, p_raw = spearmanr(df_all_uav_bridge['WH_mean'], df_all_uav_bridge['Distance'])\n",
        "\n",
        "print(f\"\\nðŸ“Œ Raw UAV/Bridge WH_mean vs Distance (n={len(df_all_uav_bridge)}):\")\n",
        "print(f\"Spearman rho = {rho_raw:.2f}, p-value = {p_raw:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "40IUO0o2_cmZ",
        "Y7YSXt38_ESx",
        "oNudvxsUpWB9",
        "RxZrDuU4_vjC",
        "3EI63KQzAJkN",
        "tUfg1ZpOAhEk",
        "Jy3-DggHpmfD",
        "pG6whlO-Bbm9",
        "qJL4HuLJBgx6",
        "A67w-IntB5_-",
        "a17Hnm4zB9db",
        "1uqMHw4qCDdc",
        "SQYoDinDCXlo",
        "ufNxc2a7Couj",
        "3nhv6Kg4CxFf",
        "oSqsu2kE0Z_J",
        "xEVN-1J8C2Z6",
        "5WgSHpUBqQ0-",
        "eQUvGutkDrZ4",
        "yhPr1O0hEadw",
        "OdWFa2W2q9A5",
        "-vrJSL-e3idW",
        "YfXf-ujIi4rK",
        "Eiyf_gFojFMo",
        "MThnYXgjjQLO",
        "vSQfiwV0jd9f",
        "x8MVWQADj8IZ",
        "lRyN25O7kOGx",
        "Dgwmr1lnkfbC",
        "X4__9ieKku8G",
        "wrtwh0V1k1gE",
        "Yl0ei_NBk9Qr",
        "MxtoeQF2lbn5",
        "drg9eT9_ll-t",
        "AQe43q3jl43u"
      ],
      "gpuType": "V28",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
